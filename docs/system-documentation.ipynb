{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69725bdf",
   "metadata": {},
   "source": [
    "# AI On-Call Agent System Documentation\n",
    "\n",
    "This comprehensive documentation covers the complete AI On-Call Agent system architecture, setup, deployment, and usage. The system provides intelligent automation for ETL infrastructure monitoring and automated issue resolution.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [System Architecture Overview](#architecture)\n",
    "2. [Service Components and Dependencies](#services)\n",
    "3. [Celery Task Queue Configuration](#celery)\n",
    "4. [Database Schema and Models](#database)\n",
    "5. [API Endpoints Documentation](#api)\n",
    "6. [Model Training in Notebooks](#training-notebooks)\n",
    "7. [Model Training via API](#training-api)\n",
    "8. [Development Environment Setup](#dev-setup)\n",
    "9. [Production Deployment Setup](#prod-setup)\n",
    "10. [Testing Endpoints with Examples](#testing)\n",
    "11. [Configuration Management](#config)\n",
    "12. [Monitoring and Logging Setup](#monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72261e",
   "metadata": {},
   "source": [
    "## 1. System Architecture Overview {#architecture}\n",
    "\n",
    "The AI On-Call Agent is a distributed system designed for intelligent ETL infrastructure monitoring and automated incident resolution. The architecture follows microservices patterns with event-driven communication.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│   Log Monitor   │    │  AI Decision    │    │ Action Engine   │\n",
    "│                 │────│    Engine       │────│                 │\n",
    "│ - File polling  │    │ - Classification│    │ - Airflow API   │\n",
    "│ - Pattern match │    │ - Confidence    │    │ - Service ops   │\n",
    "│ - Anomaly detect│    │ - Recommendations│    │ - Rollbacks     │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "         │                       │                       │\n",
    "         ▼                       ▼                       ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                     Message Queue (Redis)                       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "         │                       │                       │\n",
    "         ▼                       ▼                       ▼\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│   Knowledge     │    │   PostgreSQL    │    │    FastAPI      │\n",
    "│     Base        │    │   Database      │    │   Web Server    │\n",
    "│                 │    │                 │    │                 │\n",
    "│ - Solutions     │    │ - Incidents     │    │ - REST APIs     │\n",
    "│ - Patterns      │    │ - Training data │    │ - Webhooks      │\n",
    "│ - History       │    │ - Metrics       │    │ - Monitoring    │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "```\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "- **Backend Framework**: FastAPI (Python 3.9+)\n",
    "- **Database**: PostgreSQL 13+ with SQLAlchemy ORM\n",
    "- **Message Queue**: Redis for Celery task queue\n",
    "- **Task Queue**: Celery for background processing\n",
    "- **Machine Learning**: scikit-learn, pandas, numpy\n",
    "- **AI Integration**: OpenAI GPT for advanced analysis\n",
    "- **Monitoring**: Structured logging with JSON format\n",
    "- **Infrastructure**: Docker & Docker Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Data Flow Example\n",
    "\"\"\"\n",
    "This demonstrates the typical data flow through the system:\n",
    "\n",
    "1. Log Monitor detects anomaly\n",
    "2. Creates incident in database\n",
    "3. AI Engine analyzes and provides recommendations\n",
    "4. Action Engine executes automated fixes\n",
    "5. Results tracked and stored for training\n",
    "\"\"\"\n",
    "\n",
    "# Example incident flow\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "class IncidentSeverity(Enum):\n",
    "    CRITICAL = \"critical\"\n",
    "    HIGH = \"high\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LOW = \"low\"\n",
    "\n",
    "class IncidentStatus(Enum):\n",
    "    OPEN = \"open\"\n",
    "    IN_PROGRESS = \"in_progress\"\n",
    "    RESOLVED = \"resolved\"\n",
    "\n",
    "@dataclass\n",
    "class Incident:\n",
    "    id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    severity: IncidentSeverity\n",
    "    status: IncidentStatus\n",
    "    category: str\n",
    "    metadata: Dict[str, Any]\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"title\": self.title,\n",
    "            \"description\": self.description,\n",
    "            \"severity\": self.severity.value,\n",
    "            \"status\": self.status.value,\n",
    "            \"category\": self.category,\n",
    "            \"metadata\": self.metadata\n",
    "        }\n",
    "\n",
    "# Example incident creation\n",
    "sample_incident = Incident(\n",
    "    id=\"inc-001\",\n",
    "    title=\"Airflow DAG Failure - ETL Pipeline\",\n",
    "    description=\"ETL pipeline failed during data transformation step\",\n",
    "    severity=IncidentSeverity.HIGH,\n",
    "    status=IncidentStatus.OPEN,\n",
    "    category=\"airflow_dag\",\n",
    "    metadata={\n",
    "        \"dag_id\": \"etl_daily_pipeline\",\n",
    "        \"task_id\": \"transform_data\",\n",
    "        \"execution_date\": \"2025-07-31T10:00:00Z\",\n",
    "        \"retry_count\": 2,\n",
    "        \"max_retries\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Sample Incident:\")\n",
    "print(sample_incident.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17de2cf",
   "metadata": {},
   "source": [
    "## 2. Service Components and Dependencies {#services}\n",
    "\n",
    "### Core Services\n",
    "\n",
    "#### 2.1 Enhanced Incident Service (`enhanced_incident_service.py`)\n",
    "\n",
    "The central orchestrator for incident management, AI analysis, and automated resolution.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Incident lifecycle management (create, update, resolve)\n",
    "- AI decision engine integration\n",
    "- Automated action execution\n",
    "- Training data collection\n",
    "- Resolution tracking\n",
    "\n",
    "**Dependencies:**\n",
    "- PostgreSQL (incident storage)\n",
    "- AI Decision Engine\n",
    "- Action Service\n",
    "- Redis (for caching)\n",
    "\n",
    "#### 2.2 AI Decision Engine (`ai/__init__.py`)\n",
    "\n",
    "Provides intelligent analysis and action recommendations using machine learning models.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Incident classification\n",
    "- Confidence scoring\n",
    "- Action recommendation\n",
    "- Pattern recognition\n",
    "- Model training and updates\n",
    "\n",
    "**Models Used:**\n",
    "- RandomForestClassifier for incident categorization\n",
    "- LogisticRegression for action recommendation\n",
    "- TF-IDF Vectorizer for text analysis\n",
    "\n",
    "#### 2.3 Action Service (`actions.py`)\n",
    "\n",
    "Executes automated remediation actions across different systems.\n",
    "\n",
    "**Responsibilities:**\n",
    "- Airflow DAG restarts via REST API\n",
    "- Service restarts (K8s, Docker, systemd)\n",
    "- Database operations\n",
    "- Cache clearing\n",
    "- Spark job management\n",
    "\n",
    "#### 2.4 Log Monitor (`monitoring/__init__.py`)\n",
    "\n",
    "Real-time log monitoring and anomaly detection.\n",
    "\n",
    "**Responsibilities:**\n",
    "- File-based log polling\n",
    "- Elasticsearch integration\n",
    "- Pattern matching\n",
    "- Alert generation\n",
    "- Log archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Initialization Examples\n",
    "\n",
    "# 1. Enhanced Incident Service Setup\n",
    "class EnhancedIncidentService:\n",
    "    def __init__(self):\n",
    "        self.action_service = ActionService()\n",
    "        self.ai_engine = None  # Injected later\n",
    "        \n",
    "    def set_ai_engine(self, ai_engine):\n",
    "        \"\"\"Dependency injection for AI engine\"\"\"\n",
    "        self.ai_engine = ai_engine\n",
    "        \n",
    "    async def create_incident_with_async_resolution(self, request, auto_resolve=True):\n",
    "        \"\"\"Main entry point for incident creation with AI analysis\"\"\"\n",
    "        incident = await self.create_incident(request)\n",
    "        \n",
    "        if auto_resolve and self.ai_engine:\n",
    "            # Background task for AI analysis and resolution\n",
    "            asyncio.create_task(self._async_resolution_workflow(incident))\n",
    "            \n",
    "        return {\"incident_id\": incident.id, \"async_resolution_triggered\": True}\n",
    "\n",
    "# 2. AI Decision Engine Setup\n",
    "class AIDecisionEngine:\n",
    "    def __init__(self):\n",
    "        self.incident_classifier = None\n",
    "        self.action_recommender = None\n",
    "        self.text_vectorizer = None\n",
    "        self._load_models()\n",
    "        \n",
    "    def _load_models(self):\n",
    "        \"\"\"Load or initialize ML models\"\"\"\n",
    "        try:\n",
    "            # Load existing models or train new ones\n",
    "            self.incident_classifier = joblib.load('models/incident_classifier.pkl')\n",
    "            self.action_recommender = joblib.load('models/action_recommender.pkl')\n",
    "            self.text_vectorizer = joblib.load('models/text_vectorizer.pkl')\n",
    "        except FileNotFoundError:\n",
    "            print(\"Models not found, will train from scratch\")\n",
    "            \n",
    "    async def analyze_incident(self, incident):\n",
    "        \"\"\"Analyze incident and provide recommendations\"\"\"\n",
    "        features = self._extract_features(incident)\n",
    "        category = self.incident_classifier.predict([features])[0]\n",
    "        actions = self.action_recommender.predict([features])\n",
    "        confidence = self.incident_classifier.predict_proba([features]).max()\n",
    "        \n",
    "        return {\n",
    "            \"category\": category,\n",
    "            \"recommended_actions\": actions,\n",
    "            \"confidence_score\": confidence\n",
    "        }\n",
    "\n",
    "# 3. Action Service Setup  \n",
    "class ActionService:\n",
    "    def __init__(self):\n",
    "        self.airflow_client = None\n",
    "        self.k8s_client = None\n",
    "        self.docker_client = None\n",
    "        self._init_clients()\n",
    "        \n",
    "    def _init_clients(self):\n",
    "        \"\"\"Initialize external service clients\"\"\"\n",
    "        try:\n",
    "            import docker\n",
    "            self.docker_client = docker.from_env()\n",
    "        except Exception:\n",
    "            print(\"Docker client not available\")\n",
    "            \n",
    "    async def execute_action(self, action_type, parameters, incident_id):\n",
    "        \"\"\"Execute automated action\"\"\"\n",
    "        if action_type == \"restart_airflow_dag\":\n",
    "            return await self._restart_airflow_dag(parameters.get(\"dag_id\"))\n",
    "        elif action_type == \"restart_service\":\n",
    "            return await self._restart_service(parameters.get(\"service\"))\n",
    "        # ... other action types\n",
    "\n",
    "print(\"Service initialization examples completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6653609",
   "metadata": {},
   "source": [
    "## 3. Celery Task Queue Configuration {#celery}\n",
    "\n",
    "### Overview\n",
    "\n",
    "Celery is used for background task processing, allowing the system to handle long-running operations asynchronously without blocking API responses.\n",
    "\n",
    "### Task Types\n",
    "\n",
    "1. **AI Analysis Tasks**: Model training, incident classification\n",
    "2. **Action Execution Tasks**: Service restarts, API calls\n",
    "3. **Monitoring Tasks**: Log processing, metric collection\n",
    "4. **Maintenance Tasks**: Database cleanup, model updates\n",
    "\n",
    "### Configuration Structure\n",
    "\n",
    "```\n",
    "src/\n",
    "├── celery_app.py          # Celery application setup\n",
    "├── tasks/                 # Task definitions\n",
    "│   ├── __init__.py\n",
    "│   ├── ai_tasks.py        # AI/ML related tasks\n",
    "│   ├── action_tasks.py    # Action execution tasks\n",
    "│   └── monitoring_tasks.py # Monitoring tasks\n",
    "└── config/\n",
    "    └── celery_config.py   # Celery settings\n",
    "```\n",
    "\n",
    "### Broker Configuration\n",
    "\n",
    "- **Development**: Redis (localhost:6379)\n",
    "- **Production**: Redis Cluster or RabbitMQ\n",
    "- **Result Backend**: Same as broker for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dfc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celery Configuration Examples\n",
    "\n",
    "# 1. celery_app.py - Main Celery Application\n",
    "from celery import Celery\n",
    "import os\n",
    "\n",
    "# Create Celery instance\n",
    "celery_app = Celery(\n",
    "    \"on_call_agent\",\n",
    "    broker=os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\"),\n",
    "    backend=os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\"),\n",
    "    include=[\n",
    "        \"src.tasks.ai_tasks\",\n",
    "        \"src.tasks.action_tasks\", \n",
    "        \"src.tasks.monitoring_tasks\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "celery_app.conf.update(\n",
    "    task_serializer=\"json\",\n",
    "    accept_content=[\"json\"],\n",
    "    result_serializer=\"json\",\n",
    "    timezone=\"UTC\",\n",
    "    enable_utc=True,\n",
    "    task_track_started=True,\n",
    "    task_time_limit=30 * 60,  # 30 minutes\n",
    "    task_soft_time_limit=25 * 60,  # 25 minutes\n",
    "    worker_prefetch_multiplier=1,\n",
    "    worker_max_tasks_per_child=1000,\n",
    ")\n",
    "\n",
    "# 2. Task Examples - ai_tasks.py\n",
    "from celery import shared_task\n",
    "import asyncio\n",
    "\n",
    "@shared_task(bind=True, name=\"train_incident_classifier\")\n",
    "def train_incident_classifier_task(self, min_samples=1000):\n",
    "    \"\"\"Background task to train the incident classifier model\"\"\"\n",
    "    try:\n",
    "        self.update_state(state=\"PROGRESS\", meta={\"progress\": 0})\n",
    "        \n",
    "        # Load training data\n",
    "        from src.ai import AIDecisionEngine\n",
    "        ai_engine = AIDecisionEngine()\n",
    "        \n",
    "        self.update_state(state=\"PROGRESS\", meta={\"progress\": 25})\n",
    "        \n",
    "        # Train models\n",
    "        metrics = ai_engine.train_models(min_samples=min_samples)\n",
    "        \n",
    "        self.update_state(state=\"PROGRESS\", meta={\"progress\": 100})\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"completed\",\n",
    "            \"metrics\": metrics,\n",
    "            \"model_version\": ai_engine.model_version\n",
    "        }\n",
    "    except Exception as exc:\n",
    "        self.update_state(\n",
    "            state=\"FAILURE\",\n",
    "            meta={\"error\": str(exc), \"progress\": 0}\n",
    "        )\n",
    "        raise\n",
    "\n",
    "@shared_task(name=\"analyze_incident_async\")\n",
    "def analyze_incident_async(incident_data):\n",
    "    \"\"\"Analyze incident in background\"\"\"\n",
    "    from src.ai import AIDecisionEngine\n",
    "    ai_engine = AIDecisionEngine()\n",
    "    \n",
    "    # Convert to async call\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    try:\n",
    "        result = loop.run_until_complete(\n",
    "            ai_engine.analyze_incident(incident_data)\n",
    "        )\n",
    "        return result\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "# 3. Action Tasks - action_tasks.py\n",
    "@shared_task(bind=True, name=\"execute_airflow_restart\")\n",
    "def execute_airflow_restart_task(self, dag_id, incident_id=None):\n",
    "    \"\"\"Execute Airflow DAG restart as background task\"\"\"\n",
    "    try:\n",
    "        from src.services.actions import ActionService\n",
    "        action_service = ActionService()\n",
    "        \n",
    "        # Execute restart\n",
    "        result = asyncio.run(\n",
    "            action_service._restart_airflow_dag(dag_id)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": result[0],\n",
    "            \"error\": result[1],\n",
    "            \"dag_id\": dag_id,\n",
    "            \"incident_id\": incident_id\n",
    "        }\n",
    "    except Exception as exc:\n",
    "        self.update_state(\n",
    "            state=\"FAILURE\", \n",
    "            meta={\"error\": str(exc), \"dag_id\": dag_id}\n",
    "        )\n",
    "        raise\n",
    "\n",
    "# 4. Worker Startup Commands\n",
    "print(\"Celery worker startup commands:\")\n",
    "print(\"# Development:\")\n",
    "print(\"celery -A src.celery_app worker --loglevel=info --concurrency=2\")\n",
    "print()\n",
    "print(\"# Production:\")\n",
    "print(\"celery -A src.celery_app worker --loglevel=warning --concurrency=4\")\n",
    "print()\n",
    "print(\"# Monitor:\")\n",
    "print(\"celery -A src.celery_app flower --port=5555\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff26d7a",
   "metadata": {},
   "source": [
    "## 4. Database Schema and Models {#database}\n",
    "\n",
    "### Schema Overview\n",
    "\n",
    "The system uses PostgreSQL with SQLAlchemy ORM for data persistence. Key tables include:\n",
    "\n",
    "- **incidents**: Core incident tracking\n",
    "- **training_data**: ML model training data  \n",
    "- **knowledge_base**: Solution patterns and fixes\n",
    "- **actions**: Executed action history\n",
    "- **logs**: System and application logs\n",
    "\n",
    "### Entity Relationships\n",
    "\n",
    "```\n",
    "┌─────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│  incidents  │────│ training_data   │    │ knowledge_base  │\n",
    "│             │    │                 │    │                 │\n",
    "│ id (PK)     │    │ incident_id(FK) │    │ id (PK)         │\n",
    "│ title       │    │ features        │    │ title           │\n",
    "│ severity    │    │ target_actions  │    │ error_patterns  │\n",
    "│ status      │    │ outcome_score   │    │ solution_steps  │\n",
    "│ metadata    │    │ created_at      │    │ success_rate    │\n",
    "│ created_at  │    └─────────────────┘    └─────────────────┘\n",
    "│ resolved_at │                    \n",
    "└─────────────┘                    \n",
    "      │                            \n",
    "      ▼                            \n",
    "┌─────────────────┐                \n",
    "│    actions      │                \n",
    "│                 │                \n",
    "│ id (PK)         │                \n",
    "│ incident_id(FK) │                \n",
    "│ action_type     │                \n",
    "│ parameters      │                \n",
    "│ status          │                \n",
    "│ result          │                \n",
    "│ executed_at     │                \n",
    "└─────────────────┘                \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f170ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy Model Definitions\n",
    "\n",
    "from sqlalchemy import Column, Integer, String, DateTime, Text, Boolean, JSON, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "from datetime import datetime\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# 1. Incidents Table\n",
    "class Incident(Base):\n",
    "    __tablename__ = 'incidents'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    title = Column(String(200), nullable=False)\n",
    "    description = Column(Text, nullable=False)\n",
    "    severity = Column(String(20), nullable=False)  # critical, high, medium, low\n",
    "    service = Column(String(100))\n",
    "    status = Column(String(20), nullable=False)    # open, in_progress, resolved\n",
    "    tags = Column(JSON, default=list)\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    resolved_at = Column(DateTime)\n",
    "    \n",
    "    resolution_notes = Column(Text)\n",
    "    actions_taken = Column(JSON, default=list)\n",
    "    metadata = Column(JSON, default=dict)\n",
    "    \n",
    "    # Relationships\n",
    "    training_data = relationship(\"TrainingData\", back_populates=\"incident\")\n",
    "    actions = relationship(\"Action\", back_populates=\"incident\")\n",
    "\n",
    "# 2. Training Data Table\n",
    "class TrainingData(Base):\n",
    "    __tablename__ = 'training_data'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    incident_id = Column(String, ForeignKey('incidents.id'), nullable=False)\n",
    "    \n",
    "    # Features for ML training\n",
    "    input_features = Column(JSON, nullable=False)\n",
    "    target_actions = Column(JSON, nullable=False)\n",
    "    outcome_score = Column(Integer)  # 0-100 effectiveness score\n",
    "    resolution_time_minutes = Column(Integer)\n",
    "    \n",
    "    timestamp = Column(DateTime, default=datetime.utcnow)\n",
    "    \n",
    "    # Relationships\n",
    "    incident = relationship(\"Incident\", back_populates=\"training_data\")\n",
    "\n",
    "# 3. Knowledge Base Table\n",
    "class KnowledgeEntry(Base):\n",
    "    __tablename__ = 'knowledge_base'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    title = Column(String(200), nullable=False)\n",
    "    description = Column(Text, nullable=False)\n",
    "    category = Column(String(50), nullable=False)\n",
    "    \n",
    "    # Pattern matching\n",
    "    error_patterns = Column(JSON, nullable=False)  # List of regex patterns\n",
    "    solution_steps = Column(JSON, nullable=False)  # List of steps\n",
    "    automated_actions = Column(JSON, default=list) # Automated fix actions\n",
    "    \n",
    "    # Metadata\n",
    "    success_rate = Column(Integer, default=0)      # 0-100\n",
    "    last_used = Column(DateTime)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    created_by = Column(String(100))\n",
    "\n",
    "# 4. Actions Table  \n",
    "class Action(Base):\n",
    "    __tablename__ = 'actions'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    incident_id = Column(String, ForeignKey('incidents.id'))\n",
    "    \n",
    "    action_type = Column(String(50), nullable=False)\n",
    "    parameters = Column(JSON, default=dict)\n",
    "    status = Column(String(20), nullable=False)    # pending, running, success, failed\n",
    "    \n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    started_at = Column(DateTime)\n",
    "    completed_at = Column(DateTime)\n",
    "    \n",
    "    result = Column(JSON, default=dict)\n",
    "    error_message = Column(Text)\n",
    "    executed_by = Column(String(100))\n",
    "    is_manual = Column(Boolean, default=False)\n",
    "    \n",
    "    # Relationships\n",
    "    incident = relationship(\"Incident\", back_populates=\"actions\")\n",
    "\n",
    "# 5. Database Migration Example\n",
    "from alembic import op\n",
    "import sqlalchemy as sa\n",
    "\n",
    "def upgrade():\n",
    "    \"\"\"Create incidents table migration\"\"\"\n",
    "    op.create_table('incidents',\n",
    "        sa.Column('id', sa.String(), nullable=False),\n",
    "        sa.Column('title', sa.String(length=200), nullable=False),\n",
    "        sa.Column('description', sa.Text(), nullable=False),\n",
    "        sa.Column('severity', sa.String(length=20), nullable=False),\n",
    "        sa.Column('service', sa.String(length=100), nullable=True),\n",
    "        sa.Column('status', sa.String(length=20), nullable=False),\n",
    "        sa.Column('tags', sa.JSON(), nullable=True),\n",
    "        sa.Column('created_at', sa.DateTime(), nullable=True),\n",
    "        sa.Column('updated_at', sa.DateTime(), nullable=True),\n",
    "        sa.Column('resolved_at', sa.DateTime(), nullable=True),\n",
    "        sa.Column('resolution_notes', sa.Text(), nullable=True),\n",
    "        sa.Column('actions_taken', sa.JSON(), nullable=True),\n",
    "        sa.Column('metadata', sa.JSON(), nullable=True),\n",
    "        sa.PrimaryKeyConstraint('id')\n",
    "    )\n",
    "    \n",
    "    # Create indexes\n",
    "    op.create_index('ix_incidents_status', 'incidents', ['status'])\n",
    "    op.create_index('ix_incidents_severity', 'incidents', ['severity'])\n",
    "    op.create_index('ix_incidents_created_at', 'incidents', ['created_at'])\n",
    "\n",
    "print(\"Database models defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5839a2",
   "metadata": {},
   "source": [
    "## 5. API Endpoints Documentation {#api}\n",
    "\n",
    "### API Base URL\n",
    "- **Development**: `http://localhost:8000`\n",
    "- **Production**: `https://api.on-call-agent.yourdomain.com`\n",
    "\n",
    "### Authentication\n",
    "Currently using API key authentication (future: OAuth2/JWT)\n",
    "\n",
    "### Core Endpoints\n",
    "\n",
    "#### 5.1 Enhanced Incidents\n",
    "\n",
    "| Method | Endpoint | Description |\n",
    "|--------|----------|-------------|\n",
    "| POST | `/api/v1/enhanced-incidents/` | Create incident with auto-resolution |\n",
    "| GET | `/api/v1/enhanced-incidents/` | List incidents with filtering |\n",
    "| GET | `/api/v1/enhanced-incidents/{id}` | Get incident details |\n",
    "| PUT | `/api/v1/enhanced-incidents/{id}` | Update incident |\n",
    "\n",
    "#### 5.2 AI Training\n",
    "\n",
    "| Method | Endpoint | Description |\n",
    "|--------|----------|-------------|\n",
    "| POST | `/api/v1/ai/retrain` | Trigger model retraining |\n",
    "| GET | `/api/v1/ai/model-status` | Get current model status |\n",
    "| GET | `/api/v1/ai/training-stats` | Get training statistics |\n",
    "\n",
    "#### 5.3 Actions\n",
    "\n",
    "| Method | Endpoint | Description |\n",
    "|--------|----------|-------------|\n",
    "| GET | `/api/v1/actions/` | List executed actions |\n",
    "| POST | `/api/v1/actions/execute` | Execute manual action |\n",
    "| GET | `/api/v1/actions/{id}` | Get action details |\n",
    "\n",
    "#### 5.4 Knowledge Base\n",
    "\n",
    "| Method | Endpoint | Description |\n",
    "|--------|----------|-------------|\n",
    "| GET | `/api/v1/knowledge/` | List knowledge entries |\n",
    "| POST | `/api/v1/knowledge/` | Create knowledge entry |\n",
    "| PUT | `/api/v1/knowledge/{id}` | Update knowledge entry |\n",
    "| DELETE | `/api/v1/knowledge/{id}` | Delete knowledge entry |\n",
    "\n",
    "#### 5.5 Testing & Demo\n",
    "\n",
    "| Method | Endpoint | Description |\n",
    "|--------|----------|-------------|\n",
    "| GET | `/api/v1/testing/scenarios` | List test scenarios |\n",
    "| POST | `/api/v1/testing/simple-incident` | Create test incident |\n",
    "| POST | `/api/v1/testing/force-action/{type}` | Test specific actions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Request/Response Examples\n",
    "\n",
    "# 1. Create Enhanced Incident\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Enhanced Incident Creation\n",
    "incident_data = {\n",
    "    \"title\": \"Airflow DAG Failed\",\n",
    "    \"description\": \"The data_processing_dag failed with connection error\",\n",
    "    \"severity\": \"high\",\n",
    "    \"service_name\": \"data_pipeline\",\n",
    "    \"log_data\": \"ERROR: Failed to connect to database\",\n",
    "    \"context\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"dag_id\": \"data_processing_dag\",\n",
    "        \"task_id\": \"extract_data\",\n",
    "        \"run_id\": \"manual_2024_01_31_10_30_00\"\n",
    "    },\n",
    "    \"auto_resolve\": True,\n",
    "    \"metadata\": {\n",
    "        \"source\": \"airflow\",\n",
    "        \"dag_id\": \"data_processing_dag\",\n",
    "        \"severity_level\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# POST Request\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/api/v1/enhanced-incidents/\",\n",
    "    json=incident_data,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "print(\"Response Status:\", response.status_code)\n",
    "print(\"Response Body:\", response.json())\n",
    "\n",
    "# Expected Response:\n",
    "{\n",
    "    \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n",
    "    \"title\": \"Airflow DAG Failed\",\n",
    "    \"status\": \"resolved\",\n",
    "    \"ai_analysis\": {\n",
    "        \"classification\": \"airflow_dag_failure\",\n",
    "        \"confidence\": 0.95,\n",
    "        \"recommended_action\": \"restart_dag\"\n",
    "    },\n",
    "    \"actions_taken\": [\n",
    "        {\n",
    "            \"type\": \"restart_dag\",\n",
    "            \"status\": \"success\",\n",
    "            \"dag_id\": \"data_processing_dag\"\n",
    "        }\n",
    "    ],\n",
    "    \"created_at\": \"2024-01-31T10:30:00Z\",\n",
    "    \"resolved_at\": \"2024-01-31T10:32:15Z\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Trigger AI Model Retraining\n",
    "retrain_request = {\n",
    "    \"force_retrain\": True,\n",
    "    \"model_types\": [\"classification\", \"action_prediction\"],\n",
    "    \"include_recent_data\": True\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/api/v1/ai/retrain\",\n",
    "    json=retrain_request\n",
    ")\n",
    "\n",
    "# Expected Response:\n",
    "{\n",
    "    \"task_id\": \"retrain_models_task_123\",\n",
    "    \"status\": \"queued\",\n",
    "    \"message\": \"Model retraining task queued successfully\",\n",
    "    \"estimated_completion\": \"2024-01-31T11:00:00Z\"\n",
    "}\n",
    "\n",
    "# 3. Get Training Statistics\n",
    "response = requests.get(\"http://localhost:8000/api/v1/ai/training-stats\")\n",
    "\n",
    "# Expected Response:\n",
    "{\n",
    "    \"classification_model\": {\n",
    "        \"accuracy\": 0.94,\n",
    "        \"precision\": 0.92,\n",
    "        \"recall\": 0.89,\n",
    "        \"f1_score\": 0.91,\n",
    "        \"last_trained\": \"2024-01-30T15:30:00Z\",\n",
    "        \"training_samples\": 1540\n",
    "    },\n",
    "    \"action_prediction_model\": {\n",
    "        \"accuracy\": 0.87,\n",
    "        \"precision\": 0.85,\n",
    "        \"recall\": 0.88,\n",
    "        \"f1_score\": 0.86,\n",
    "        \"last_trained\": \"2024-01-30T15:30:00Z\",\n",
    "        \"training_samples\": 890\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. Test Simple Incident Creation (for testing)\n",
    "test_incident = {\n",
    "    \"title\": \"Test DAG Issue\",\n",
    "    \"description\": \"Testing fallback logic\",\n",
    "    \"severity\": \"medium\",\n",
    "    \"service_name\": \"test_service\",\n",
    "    \"log_data\": \"Test error message\",\n",
    "    \"context\": {\"environment\": \"development\"},\n",
    "    \"auto_resolve\": True,\n",
    "    \"metadata\": {\"dag_id\": \"simple_test_dag\"}\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/api/v1/testing/simple-incident\",\n",
    "    json=test_incident\n",
    ")\n",
    "\n",
    "print(\"Test Response:\", response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aaccef",
   "metadata": {},
   "source": [
    "## 6. Model Training in Notebooks {#training-notebooks}\n",
    "\n",
    "### 6.1 Training Data Collection\n",
    "\n",
    "The system collects training data from resolved incidents to continuously improve AI models:\n",
    "\n",
    "- **Classification Training**: Incident descriptions → problem categories\n",
    "- **Action Prediction**: Problem types → successful resolution actions\n",
    "- **Severity Assessment**: Log patterns → severity levels\n",
    "\n",
    "### 6.2 Notebook-Based Training Workflow\n",
    "\n",
    "#### Location\n",
    "- Training notebooks: `notebooks/training/`\n",
    "- Data preparation: `notebooks/data_preparation/`\n",
    "- Model evaluation: `notebooks/evaluation/`\n",
    "\n",
    "#### Key Notebooks\n",
    "\n",
    "1. **`01_data_preparation.ipynb`**: Data cleaning and feature engineering\n",
    "2. **`02_classification_training.ipynb`**: Incident classification model\n",
    "3. **`03_action_prediction.ipynb`**: Action recommendation model\n",
    "4. **`04_model_evaluation.ipynb`**: Performance analysis and validation\n",
    "5. **`05_hyperparameter_tuning.ipynb`**: Model optimization\n",
    "\n",
    "### 6.3 Training Pipeline Components\n",
    "\n",
    "#### Data Sources\n",
    "- PostgreSQL incidents table\n",
    "- Historical action success rates\n",
    "- Log pattern libraries\n",
    "- Knowledge base entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Code Examples for Notebooks\n",
    "\n",
    "# Example from 02_classification_training.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 1. Data Loading and Preparation\n",
    "def load_training_data():\n",
    "    \"\"\"Load training data from PostgreSQL\"\"\"\n",
    "    from sqlalchemy import create_engine\n",
    "    \n",
    "    engine = create_engine(os.getenv('DATABASE_URL'))\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        i.title, \n",
    "        i.description, \n",
    "        i.log_data,\n",
    "        i.severity,\n",
    "        td.problem_category,\n",
    "        td.action_taken,\n",
    "        td.success_rate\n",
    "    FROM incidents i\n",
    "    JOIN training_data td ON i.id = td.incident_id\n",
    "    WHERE i.status = 'resolved'\n",
    "    AND td.feedback_score >= 0.7\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df\n",
    "\n",
    "# 2. Feature Engineering\n",
    "def prepare_features(df):\n",
    "    \"\"\"Create features for model training\"\"\"\n",
    "    # Combine text fields\n",
    "    df['combined_text'] = (\n",
    "        df['title'].fillna('') + ' ' + \n",
    "        df['description'].fillna('') + ' ' + \n",
    "        df['log_data'].fillna('')\n",
    "    )\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    X_text = vectorizer.fit_transform(df['combined_text'])\n",
    "    \n",
    "    # Save vectorizer\n",
    "    joblib.dump(vectorizer, 'models/tfidf_vectorizer.pkl')\n",
    "    \n",
    "    return X_text, df['problem_category']\n",
    "\n",
    "# 3. Model Training\n",
    "def train_classification_model():\n",
    "    \"\"\"Train incident classification model\"\"\"\n",
    "    df = load_training_data()\n",
    "    X, y = prepare_features(df)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train Random Forest\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, 'models/classification_model.pkl')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 4. Action Prediction Model Training\n",
    "def train_action_prediction_model():\n",
    "    \"\"\"Train action recommendation model\"\"\"\n",
    "    df = load_training_data()\n",
    "    \n",
    "    # Features: problem_category + severity + historical success\n",
    "    feature_cols = ['problem_category', 'severity']\n",
    "    X = pd.get_dummies(df[feature_cols])\n",
    "    y = df['action_taken']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, 'models/action_prediction_model.pkl')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 5. Model Validation\n",
    "def validate_models():\n",
    "    \"\"\"Cross-validation and performance metrics\"\"\"\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    df = load_training_data()\n",
    "    X, y = prepare_features(df)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='f1_weighted')\n",
    "    \n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean F1 score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Example usage in notebook:\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training Classification Model...\")\n",
    "    classification_model = train_classification_model()\n",
    "    \n",
    "    print(\"Training Action Prediction Model...\")\n",
    "    action_model = train_action_prediction_model()\n",
    "    \n",
    "    print(\"Validating Models...\")\n",
    "    validation_scores = validate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329272e",
   "metadata": {},
   "source": [
    "## 7. Model Training via API {#api-training}\n",
    "\n",
    "### 7.1 Automated Training Pipeline\n",
    "\n",
    "The system provides REST API endpoints for triggering model retraining without manual notebook execution.\n",
    "\n",
    "#### Training Triggers\n",
    "- **Scheduled**: Cron-based automatic retraining (daily/weekly)\n",
    "- **Threshold-based**: Retrain when model performance drops below threshold\n",
    "- **Manual**: On-demand training via API call\n",
    "- **Data-driven**: Retrain when new training samples reach minimum threshold\n",
    "\n",
    "### 7.2 Training API Endpoints\n",
    "\n",
    "#### Trigger Training\n",
    "```http\n",
    "POST /api/v1/ai/retrain\n",
    "Content-Type: application/json\n",
    "\n",
    "{\n",
    "    \"force_retrain\": true,\n",
    "    \"model_types\": [\"classification\", \"action_prediction\"],\n",
    "    \"include_recent_data\": true,\n",
    "    \"training_config\": {\n",
    "        \"test_size\": 0.2,\n",
    "        \"cv_folds\": 5,\n",
    "        \"min_samples\": 100\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Monitor Training Progress\n",
    "```http\n",
    "GET /api/v1/ai/training-status/{task_id}\n",
    "```\n",
    "\n",
    "#### Get Model Performance\n",
    "```http\n",
    "GET /api/v1/ai/model-metrics\n",
    "```\n",
    "\n",
    "### 7.3 Training Configuration\n",
    "\n",
    "#### Environment Variables\n",
    "- `MIN_TRAINING_SAMPLES`: Minimum samples required for training (default: 100)\n",
    "- `RETRAIN_THRESHOLD`: Performance threshold for automatic retraining (default: 0.85)\n",
    "- `TRAINING_SCHEDULE`: Cron expression for scheduled training\n",
    "- `MODEL_BACKUP_COUNT`: Number of model versions to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Training Implementation\n",
    "\n",
    "# FastAPI endpoint for model retraining\n",
    "from fastapi import APIRouter, BackgroundTasks, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import uuid\n",
    "\n",
    "router = APIRouter(prefix=\"/api/v1/ai\", tags=[\"AI Training\"])\n",
    "\n",
    "class TrainingRequest(BaseModel):\n",
    "    force_retrain: bool = False\n",
    "    model_types: List[str] = [\"classification\", \"action_prediction\"]\n",
    "    include_recent_data: bool = True\n",
    "    training_config: Optional[dict] = None\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    task_id: str\n",
    "    status: str\n",
    "    message: str\n",
    "    estimated_completion: str\n",
    "\n",
    "@router.post(\"/retrain\", response_model=TrainingResponse)\n",
    "async def trigger_model_retraining(\n",
    "    request: TrainingRequest,\n",
    "    background_tasks: BackgroundTasks\n",
    "):\n",
    "    \"\"\"Trigger model retraining via Celery task\"\"\"\n",
    "    \n",
    "    # Generate unique task ID\n",
    "    task_id = f\"retrain_models_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    # Queue Celery task\n",
    "    from services.ai_service import retrain_models_task\n",
    "    \n",
    "    task = retrain_models_task.delay(\n",
    "        model_types=request.model_types,\n",
    "        force_retrain=request.force_retrain,\n",
    "        include_recent_data=request.include_recent_data,\n",
    "        config=request.training_config or {}\n",
    "    )\n",
    "    \n",
    "    return TrainingResponse(\n",
    "        task_id=task.id,\n",
    "        status=\"queued\",\n",
    "        message=\"Model retraining task queued successfully\",\n",
    "        estimated_completion=\"2024-01-31T11:00:00Z\"\n",
    "    )\n",
    "\n",
    "# Celery task implementation\n",
    "from celery import Celery\n",
    "from services.database import get_db\n",
    "from ai.model_trainer import ModelTrainer\n",
    "import logging\n",
    "\n",
    "@celery_app.task(bind=True)\n",
    "def retrain_models_task(self, model_types, force_retrain=False, include_recent_data=True, config=None):\n",
    "    \"\"\"Background task for model retraining\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Update task status\n",
    "        self.update_state(state='PROGRESS', meta={'status': 'Loading training data'})\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = ModelTrainer(config=config)\n",
    "        \n",
    "        # Load training data\n",
    "        training_data = trainer.load_training_data(include_recent=include_recent_data)\n",
    "        \n",
    "        self.update_state(state='PROGRESS', meta={'status': 'Training models'})\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Train each requested model type\n",
    "        for model_type in model_types:\n",
    "            if model_type == \"classification\":\n",
    "                results['classification'] = trainer.train_classification_model(training_data)\n",
    "            elif model_type == \"action_prediction\":\n",
    "                results['action_prediction'] = trainer.train_action_prediction_model(training_data)\n",
    "        \n",
    "        # Save models\n",
    "        self.update_state(state='PROGRESS', meta={'status': 'Saving models'})\n",
    "        trainer.save_models(results)\n",
    "        \n",
    "        # Update model registry\n",
    "        trainer.update_model_registry(results)\n",
    "        \n",
    "        return {\n",
    "            'status': 'completed',\n",
    "            'results': results,\n",
    "            'training_samples': len(training_data),\n",
    "            'completion_time': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as exc:\n",
    "        logging.error(f\"Model training failed: {exc}\")\n",
    "        self.update_state(\n",
    "            state='FAILURE',\n",
    "            meta={'status': 'failed', 'error': str(exc)}\n",
    "        )\n",
    "        raise\n",
    "\n",
    "# Model Trainer Class\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or {}\n",
    "        self.db = get_db()\n",
    "    \n",
    "    def load_training_data(self, include_recent=True):\n",
    "        \"\"\"Load training data from database\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT i.*, td.* FROM incidents i\n",
    "        JOIN training_data td ON i.id = td.incident_id\n",
    "        WHERE i.status = 'resolved'\n",
    "        \"\"\"\n",
    "        \n",
    "        if include_recent:\n",
    "            query += \" AND i.created_at >= NOW() - INTERVAL '30 days'\"\n",
    "        \n",
    "        return pd.read_sql(query, self.db.connection())\n",
    "    \n",
    "    def train_classification_model(self, data):\n",
    "        \"\"\"Train incident classification model\"\"\"\n",
    "        # Implementation similar to notebook version\n",
    "        # but optimized for production use\n",
    "        pass\n",
    "    \n",
    "    def save_models(self, results):\n",
    "        \"\"\"Save trained models with versioning\"\"\"\n",
    "        for model_type, model_data in results.items():\n",
    "            model_path = f\"models/{model_type}_v{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "            joblib.dump(model_data['model'], model_path)\n",
    "            \n",
    "            # Update model registry in database\n",
    "            self.update_model_registry(model_type, model_path, model_data['metrics'])\n",
    "\n",
    "# Usage example\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "async def retrain_models_example():\n",
    "    \"\"\"Example of triggering model retraining\"\"\"\n",
    "    \n",
    "    # API call to trigger retraining\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/api/v1/ai/retrain\",\n",
    "        json={\n",
    "            \"force_retrain\": True,\n",
    "            \"model_types\": [\"classification\", \"action_prediction\"],\n",
    "            \"include_recent_data\": True,\n",
    "            \"training_config\": {\n",
    "                \"test_size\": 0.2,\n",
    "                \"cv_folds\": 5,\n",
    "                \"min_samples\": 100\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    task_id = response.json()[\"task_id\"]\n",
    "    print(f\"Training task queued: {task_id}\")\n",
    "    \n",
    "    # Poll for completion\n",
    "    while True:\n",
    "        status_response = requests.get(f\"http://localhost:8000/api/v1/ai/training-status/{task_id}\")\n",
    "        status = status_response.json()\n",
    "        \n",
    "        if status[\"state\"] in [\"SUCCESS\", \"FAILURE\"]:\n",
    "            print(f\"Training completed with status: {status['state']}\")\n",
    "            break\n",
    "        \n",
    "        await asyncio.sleep(30)  # Check every 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92c6cf",
   "metadata": {},
   "source": [
    "## 8. Development Environment Setup {#dev-setup}\n",
    "\n",
    "### 8.1 Prerequisites\n",
    "\n",
    "#### System Requirements\n",
    "- **Python**: 3.9+ \n",
    "- **Docker**: 20.10+\n",
    "- **Docker Compose**: 2.0+\n",
    "- **Git**: Latest version\n",
    "- **Node.js**: 16+ (for frontend development)\n",
    "\n",
    "#### Development Tools\n",
    "- VS Code with Python extension\n",
    "- Jupyter Lab/Notebook\n",
    "- PostgreSQL client (psql or GUI tool)\n",
    "- Redis CLI\n",
    "- Postman or similar API testing tool\n",
    "\n",
    "### 8.2 Local Development Setup\n",
    "\n",
    "#### Step 1: Repository Setup\n",
    "```bash\n",
    "# Clone repository\n",
    "git clone https://github.com/your-org/on-call-agent.git\n",
    "cd on-call-agent\n",
    "\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "pip install -r requirements-dev.txt\n",
    "```\n",
    "\n",
    "#### Step 2: Environment Configuration\n",
    "```bash\n",
    "# Copy environment template\n",
    "cp .env.example .env\n",
    "\n",
    "# Edit .env with your settings\n",
    "# Key variables:\n",
    "DATABASE_URL=postgresql://user:password@localhost:5432/oncollagent_dev\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "AIRFLOW_API_URL=http://localhost:8080/api/v1\n",
    "OPENAI_API_KEY=your_openai_key\n",
    "DEFAULT_DAG_ID=simple_test_dag\n",
    "```\n",
    "\n",
    "#### Step 3: Database Setup\n",
    "```bash\n",
    "# Start PostgreSQL and Redis\n",
    "docker-compose up -d postgres redis\n",
    "\n",
    "# Run database migrations\n",
    "alembic upgrade head\n",
    "\n",
    "# Seed test data (optional)\n",
    "python scripts/seed_test_data.py\n",
    "```\n",
    "\n",
    "### 8.3 Development Services\n",
    "\n",
    "#### Service Start Order\n",
    "1. **Database & Cache**: PostgreSQL, Redis\n",
    "2. **Background Workers**: Celery worker, Celery beat\n",
    "3. **API Server**: FastAPI application\n",
    "4. **Monitoring**: Airflow (if testing integrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686123f8",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Development Startup Commands\n",
    "\n",
    "# 1. Start Infrastructure Services\n",
    "docker-compose up -d postgres redis\n",
    "\n",
    "# 2. Start Celery Worker (Terminal 1)\n",
    "celery -A app.core.celery worker --loglevel=info --concurrency=4\n",
    "\n",
    "# 3. Start Celery Beat Scheduler (Terminal 2)  \n",
    "celery -A app.core.celery beat --loglevel=info\n",
    "\n",
    "# 4. Start FastAPI Development Server (Terminal 3)\n",
    "uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n",
    "\n",
    "# 5. Optional: Start Jupyter Lab for notebook development\n",
    "jupyter lab --port 8888 --no-browser\n",
    "\n",
    "# Alternative: One-command startup using Docker Compose\n",
    "docker-compose up --build\n",
    "\n",
    "# Development helper scripts\n",
    "chmod +x scripts/dev-setup.sh\n",
    "./scripts/dev-setup.sh\n",
    "\n",
    "# Run tests\n",
    "pytest tests/ -v --cov=app\n",
    "\n",
    "# Database operations\n",
    "alembic revision --autogenerate -m \"description\"\n",
    "alembic upgrade head\n",
    "alembic downgrade -1\n",
    "\n",
    "# Lint and format code\n",
    "black app/ tests/\n",
    "isort app/ tests/\n",
    "flake8 app/ tests/\n",
    "\n",
    "# View logs\n",
    "docker-compose logs -f api\n",
    "docker-compose logs -f worker\n",
    "docker-compose logs -f postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b0e07",
   "metadata": {},
   "source": [
    "## 9. Production Deployment {#production}\n",
    "\n",
    "### 9.1 Infrastructure Requirements\n",
    "\n",
    "#### Compute Resources\n",
    "- **API Server**: 2+ CPU cores, 4GB+ RAM\n",
    "- **Worker Nodes**: 1+ CPU core, 2GB+ RAM per worker\n",
    "- **Database**: 2+ CPU cores, 8GB+ RAM, SSD storage\n",
    "- **Cache**: 1 CPU core, 1GB+ RAM\n",
    "\n",
    "#### External Dependencies\n",
    "- PostgreSQL 13+ (managed service recommended)\n",
    "- Redis 6+ (managed service recommended)\n",
    "- Load balancer (nginx, ALB, etc.)\n",
    "- SSL certificates\n",
    "- Monitoring & logging infrastructure\n",
    "\n",
    "### 9.2 Container Orchestration\n",
    "\n",
    "#### Docker Compose (Simple Deployment)\n",
    "```yaml\n",
    "# docker-compose.prod.yml\n",
    "version: '3.8'\n",
    "services:\n",
    "  api:\n",
    "    image: on-call-agent:latest\n",
    "    environment:\n",
    "      - ENV=production\n",
    "      - DATABASE_URL=${DATABASE_URL}\n",
    "      - REDIS_URL=${REDIS_URL}\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    depends_on:\n",
    "      - postgres\n",
    "      - redis\n",
    "    restart: unless-stopped\n",
    "    \n",
    "  worker:\n",
    "    image: on-call-agent:latest\n",
    "    command: celery -A app.core.celery worker --loglevel=info\n",
    "    environment:\n",
    "      - ENV=production\n",
    "      - DATABASE_URL=${DATABASE_URL}\n",
    "      - REDIS_URL=${REDIS_URL}\n",
    "    depends_on:\n",
    "      - postgres\n",
    "      - redis\n",
    "    restart: unless-stopped\n",
    "    \n",
    "  beat:\n",
    "    image: on-call-agent:latest\n",
    "    command: celery -A app.core.celery beat --loglevel=info\n",
    "    environment:\n",
    "      - ENV=production\n",
    "      - DATABASE_URL=${DATABASE_URL}\n",
    "      - REDIS_URL=${REDIS_URL}\n",
    "    depends_on:\n",
    "      - postgres\n",
    "      - redis\n",
    "    restart: unless-stopped\n",
    "```\n",
    "\n",
    "#### Kubernetes (Advanced Deployment)\n",
    "- Helm charts in `deploy/kubernetes/`\n",
    "- Horizontal Pod Autoscaling (HPA)\n",
    "- Resource quotas and limits\n",
    "- Health checks and readiness probes\n",
    "- Secret management for sensitive data\n",
    "\n",
    "### 9.3 Environment Configuration\n",
    "\n",
    "#### Production Environment Variables\n",
    "```bash\n",
    "# Application\n",
    "ENV=production\n",
    "DEBUG=false\n",
    "LOG_LEVEL=INFO\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=postgresql://user:pass@prod-db:5432/oncollagent\n",
    "DATABASE_POOL_SIZE=20\n",
    "DATABASE_MAX_OVERFLOW=10\n",
    "\n",
    "# Redis\n",
    "REDIS_URL=redis://prod-redis:6379/0\n",
    "REDIS_POOL_SIZE=10\n",
    "\n",
    "# Security\n",
    "SECRET_KEY=your-secure-secret-key\n",
    "API_KEY_HEADER=X-API-Key\n",
    "ALLOWED_HOSTS=api.on-call-agent.com\n",
    "\n",
    "# External APIs\n",
    "AIRFLOW_API_URL=https://airflow.company.com/api/v1\n",
    "AIRFLOW_USERNAME=on_call_agent\n",
    "AIRFLOW_PASSWORD=secure_password\n",
    "OPENAI_API_KEY=your_openai_key\n",
    "\n",
    "# Monitoring\n",
    "SENTRY_DSN=https://your-sentry-dsn\n",
    "PROMETHEUS_METRICS=true\n",
    "LOG_FORMAT=json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab33f2b",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Production Deployment Commands\n",
    "\n",
    "# 1. Build Production Image\n",
    "docker build -t on-call-agent:latest -f Dockerfile.prod .\n",
    "\n",
    "# 2. Deploy with Docker Compose\n",
    "docker-compose -f docker-compose.prod.yml up -d\n",
    "\n",
    "# 3. Database Migration (Production)\n",
    "docker-compose -f docker-compose.prod.yml exec api alembic upgrade head\n",
    "\n",
    "# 4. Health Check\n",
    "curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# 5. Kubernetes Deployment (if using K8s)\n",
    "helm install on-call-agent ./deploy/helm/on-call-agent \\\n",
    "  --values ./deploy/helm/values.prod.yaml \\\n",
    "  --namespace production\n",
    "\n",
    "# 6. Scaling Workers\n",
    "docker-compose -f docker-compose.prod.yml up -d --scale worker=3\n",
    "\n",
    "# 7. Log Monitoring\n",
    "docker-compose -f docker-compose.prod.yml logs -f api worker beat\n",
    "\n",
    "# 8. Backup Database\n",
    "pg_dump $DATABASE_URL > backup_$(date +%Y%m%d_%H%M%S).sql\n",
    "\n",
    "# 9. Rolling Update\n",
    "docker-compose -f docker-compose.prod.yml pull\n",
    "docker-compose -f docker-compose.prod.yml up -d --no-deps api worker\n",
    "\n",
    "# 10. Production Testing\n",
    "pytest tests/integration/ --env=production\n",
    "\n",
    "# SSL Certificate Setup (Let's Encrypt)\n",
    "certbot --nginx -d api.on-call-agent.com\n",
    "\n",
    "# Nginx Configuration\n",
    "upstream on_call_api {\n",
    "    server 127.0.0.1:8000;\n",
    "}\n",
    "\n",
    "server {\n",
    "    listen 443 ssl;\n",
    "    server_name api.on-call-agent.com;\n",
    "    \n",
    "    ssl_certificate /etc/letsencrypt/live/api.on-call-agent.com/fullchain.pem;\n",
    "    ssl_certificate_key /etc/letsencrypt/live/api.on-call-agent.com/privkey.pem;\n",
    "    \n",
    "    location / {\n",
    "        proxy_pass http://on_call_api;\n",
    "        proxy_set_header Host $host;\n",
    "        proxy_set_header X-Real-IP $remote_addr;\n",
    "    }\n",
    "}\n",
    "\n",
    "# Monitoring Commands\n",
    "# Prometheus metrics endpoint\n",
    "curl http://localhost:8000/metrics\n",
    "\n",
    "# Application logs\n",
    "tail -f /var/log/on-call-agent/app.log\n",
    "\n",
    "# System monitoring\n",
    "htop  # CPU/Memory usage\n",
    "iostat  # Disk I/O\n",
    "netstat -tulpn  # Network connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e664a",
   "metadata": {},
   "source": [
    "## 10. Testing Guide {#testing}\n",
    "\n",
    "### 10.1 Testing Strategy\n",
    "\n",
    "#### Test Types\n",
    "- **Unit Tests**: Individual functions and classes\n",
    "- **Integration Tests**: Service interactions\n",
    "- **API Tests**: HTTP endpoint validation\n",
    "- **End-to-End Tests**: Complete workflow testing\n",
    "- **Performance Tests**: Load and stress testing\n",
    "\n",
    "#### Test Structure\n",
    "```\n",
    "tests/\n",
    "├── unit/\n",
    "│   ├── test_ai_service.py\n",
    "│   ├── test_enhanced_incident_service.py\n",
    "│   └── test_action_service.py\n",
    "├── integration/\n",
    "│   ├── test_database_operations.py\n",
    "│   ├── test_celery_tasks.py\n",
    "│   └── test_airflow_integration.py\n",
    "├── api/\n",
    "│   ├── test_incident_endpoints.py\n",
    "│   ├── test_ai_endpoints.py\n",
    "│   └── test_auth_endpoints.py\n",
    "└── e2e/\n",
    "    ├── test_incident_workflow.py\n",
    "    └── test_training_pipeline.py\n",
    "```\n",
    "\n",
    "### 10.2 API Endpoint Testing\n",
    "\n",
    "#### Test Environment Setup\n",
    "```bash\n",
    "# Start test environment\n",
    "docker-compose -f docker-compose.test.yml up -d\n",
    "\n",
    "# Run API tests\n",
    "pytest tests/api/ -v --env=test\n",
    "```\n",
    "\n",
    "#### Testing Tools\n",
    "- **pytest**: Test framework\n",
    "- **httpx**: Async HTTP client\n",
    "- **pytest-asyncio**: Async test support\n",
    "- **factories**: Test data generation\n",
    "- **fixtures**: Test setup/teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Testing Examples\n",
    "\n",
    "# 1. API Endpoint Testing\n",
    "import pytest\n",
    "import httpx\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "@pytest.fixture\n",
    "async def client():\n",
    "    \"\"\"Create test client\"\"\"\n",
    "    from app.main import app\n",
    "    async with httpx.AsyncClient(app=app, base_url=\"http://test\") as client:\n",
    "        yield client\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_incident_data():\n",
    "    \"\"\"Sample incident data for testing\"\"\"\n",
    "    return {\n",
    "        \"title\": \"Test DAG Failed\",\n",
    "        \"description\": \"Test incident for API validation\",\n",
    "        \"severity\": \"high\",\n",
    "        \"service_name\": \"test_service\",\n",
    "        \"log_data\": \"ERROR: Test error message\",\n",
    "        \"context\": {\n",
    "            \"environment\": \"test\",\n",
    "            \"dag_id\": \"simple_test_dag\",\n",
    "            \"task_id\": \"test_task\"\n",
    "        },\n",
    "        \"auto_resolve\": True,\n",
    "        \"metadata\": {\n",
    "            \"source\": \"airflow\",\n",
    "            \"dag_id\": \"simple_test_dag\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test Enhanced Incidents Endpoint\n",
    "@pytest.mark.asyncio\n",
    "async def test_create_enhanced_incident(client, sample_incident_data):\n",
    "    \"\"\"Test incident creation with auto-resolution\"\"\"\n",
    "    response = await client.post(\n",
    "        \"/api/v1/enhanced-incidents/\",\n",
    "        json=sample_incident_data\n",
    "    )\n",
    "    \n",
    "    assert response.status_code == 201\n",
    "    data = response.json()\n",
    "    assert data[\"title\"] == sample_incident_data[\"title\"]\n",
    "    assert data[\"status\"] in [\"processing\", \"resolved\"]\n",
    "    assert \"ai_analysis\" in data\n",
    "    assert \"id\" in data\n",
    "    \n",
    "    return data[\"id\"]\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_get_incident_details(client, sample_incident_data):\n",
    "    \"\"\"Test retrieving incident details\"\"\"\n",
    "    # First create incident\n",
    "    create_response = await client.post(\n",
    "        \"/api/v1/enhanced-incidents/\",\n",
    "        json=sample_incident_data\n",
    "    )\n",
    "    incident_id = create_response.json()[\"id\"]\n",
    "    \n",
    "    # Then retrieve it\n",
    "    response = await client.get(f\"/api/v1/enhanced-incidents/{incident_id}\")\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert data[\"id\"] == incident_id\n",
    "    assert \"ai_analysis\" in data\n",
    "    assert \"actions_taken\" in data\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_list_incidents_with_filters(client):\n",
    "    \"\"\"Test incident listing with filters\"\"\"\n",
    "    response = await client.get(\n",
    "        \"/api/v1/enhanced-incidents/\",\n",
    "        params={\n",
    "            \"severity\": \"high\",\n",
    "            \"status\": \"resolved\",\n",
    "            \"limit\": 10\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"items\" in data\n",
    "    assert \"total\" in data\n",
    "    assert len(data[\"items\"]) <= 10\n",
    "\n",
    "# Test AI Training Endpoints\n",
    "@pytest.mark.asyncio\n",
    "async def test_trigger_model_retraining(client):\n",
    "    \"\"\"Test model retraining trigger\"\"\"\n",
    "    training_request = {\n",
    "        \"force_retrain\": True,\n",
    "        \"model_types\": [\"classification\"],\n",
    "        \"include_recent_data\": True,\n",
    "        \"training_config\": {\n",
    "            \"test_size\": 0.2,\n",
    "            \"min_samples\": 10  # Lower for testing\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = await client.post(\n",
    "        \"/api/v1/ai/retrain\",\n",
    "        json=training_request\n",
    "    )\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"task_id\" in data\n",
    "    assert data[\"status\"] == \"queued\"\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_get_model_metrics(client):\n",
    "    \"\"\"Test model performance metrics\"\"\"\n",
    "    response = await client.get(\"/api/v1/ai/model-metrics\")\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"classification_model\" in data or \"no_models_trained\" in data\n",
    "\n",
    "# Test Action Endpoints\n",
    "@pytest.mark.asyncio\n",
    "async def test_list_actions(client):\n",
    "    \"\"\"Test action listing\"\"\"\n",
    "    response = await client.get(\"/api/v1/actions/\")\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert isinstance(data, list)\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_execute_manual_action(client):\n",
    "    \"\"\"Test manual action execution\"\"\"\n",
    "    action_request = {\n",
    "        \"action_type\": \"restart_dag\",\n",
    "        \"parameters\": {\n",
    "            \"dag_id\": \"simple_test_dag\"\n",
    "        },\n",
    "        \"reason\": \"Manual test execution\"\n",
    "    }\n",
    "    \n",
    "    response = await client.post(\n",
    "        \"/api/v1/actions/execute\",\n",
    "        json=action_request\n",
    "    )\n",
    "    \n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"action_id\" in data\n",
    "    assert data[\"status\"] in [\"queued\", \"executing\"]\n",
    "\n",
    "# Integration Tests\n",
    "@pytest.mark.asyncio\n",
    "async def test_complete_incident_workflow(client, sample_incident_data):\n",
    "    \"\"\"Test complete incident resolution workflow\"\"\"\n",
    "    \n",
    "    # 1. Create incident\n",
    "    create_response = await client.post(\n",
    "        \"/api/v1/enhanced-incidents/\",\n",
    "        json=sample_incident_data\n",
    "    )\n",
    "    incident_id = create_response.json()[\"id\"]\n",
    "    \n",
    "    # 2. Wait for processing (in real tests, use polling)\n",
    "    await asyncio.sleep(2)\n",
    "    \n",
    "    # 3. Check incident status\n",
    "    status_response = await client.get(f\"/api/v1/enhanced-incidents/{incident_id}\")\n",
    "    incident_data = status_response.json()\n",
    "    \n",
    "    # 4. Verify AI analysis was performed\n",
    "    assert \"ai_analysis\" in incident_data\n",
    "    assert incident_data[\"ai_analysis\"][\"classification\"] is not None\n",
    "    \n",
    "    # 5. Verify actions were taken if auto_resolve=True\n",
    "    if sample_incident_data[\"auto_resolve\"]:\n",
    "        assert len(incident_data[\"actions_taken\"]) > 0\n",
    "        assert incident_data[\"status\"] in [\"resolved\", \"processing\"]\n",
    "\n",
    "# Performance Testing\n",
    "@pytest.mark.asyncio\n",
    "async def test_api_performance(client, sample_incident_data):\n",
    "    \"\"\"Test API response times\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create multiple incidents concurrently\n",
    "    tasks = []\n",
    "    for i in range(5):\n",
    "        task = client.post(\n",
    "            \"/api/v1/enhanced-incidents/\",\n",
    "            json={**sample_incident_data, \"title\": f\"Test Incident {i}\"}\n",
    "        )\n",
    "        tasks.append(task)\n",
    "    \n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Verify all requests succeeded\n",
    "    for response in responses:\n",
    "        assert response.status_code == 201\n",
    "    \n",
    "    # Verify reasonable performance (adjust threshold as needed)\n",
    "    assert total_time < 10.0  # Should complete within 10 seconds\n",
    "\n",
    "# Database Testing\n",
    "def test_database_connection():\n",
    "    \"\"\"Test database connectivity\"\"\"\n",
    "    from app.core.database import get_db\n",
    "    \n",
    "    db = next(get_db())\n",
    "    result = db.execute(\"SELECT 1\").scalar()\n",
    "    assert result == 1\n",
    "\n",
    "def test_incident_model_creation():\n",
    "    \"\"\"Test incident model CRUD operations\"\"\"\n",
    "    from app.models.incident import Incident\n",
    "    from app.core.database import get_db\n",
    "    \n",
    "    db = next(get_db())\n",
    "    \n",
    "    # Create\n",
    "    incident = Incident(\n",
    "        title=\"Test Incident\",\n",
    "        description=\"Test description\",\n",
    "        severity=\"medium\",\n",
    "        service_name=\"test_service\"\n",
    "    )\n",
    "    db.add(incident)\n",
    "    db.commit()\n",
    "    \n",
    "    # Read\n",
    "    retrieved = db.query(Incident).filter(Incident.title == \"Test Incident\").first()\n",
    "    assert retrieved is not None\n",
    "    assert retrieved.severity == \"medium\"\n",
    "    \n",
    "    # Update\n",
    "    retrieved.status = \"resolved\"\n",
    "    db.commit()\n",
    "    \n",
    "    # Verify update\n",
    "    updated = db.query(Incident).filter(Incident.id == retrieved.id).first()\n",
    "    assert updated.status == \"resolved\"\n",
    "    \n",
    "    # Delete\n",
    "    db.delete(updated)\n",
    "    db.commit()\n",
    "\n",
    "# Manual Testing Scripts\n",
    "def manual_test_simple_incident():\n",
    "    \"\"\"Manual test script for simple incident creation\"\"\"\n",
    "    import requests\n",
    "    \n",
    "    test_data = {\n",
    "        \"title\": \"Manual Test DAG Issue\",\n",
    "        \"description\": \"Testing the system manually\",\n",
    "        \"severity\": \"medium\",\n",
    "        \"service_name\": \"test_service\",\n",
    "        \"log_data\": \"ERROR: Manual test error\",\n",
    "        \"context\": {\"environment\": \"development\"},\n",
    "        \"auto_resolve\": True,\n",
    "        \"metadata\": {\"dag_id\": \"simple_test_dag\"}\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/api/v1/enhanced-incidents/\",\n",
    "        json=test_data\n",
    "    )\n",
    "    \n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    print(f\"Response: {response.json()}\")\n",
    "    \n",
    "    return response.json()[\"id\"] if response.status_code == 201 else None\n",
    "\n",
    "# Load Testing Example\n",
    "async def load_test_api():\n",
    "    \"\"\"Simple load test for API endpoints\"\"\"\n",
    "    import aiohttp\n",
    "    import asyncio\n",
    "    \n",
    "    async def create_incident(session, incident_num):\n",
    "        async with session.post(\n",
    "            \"http://localhost:8000/api/v1/enhanced-incidents/\",\n",
    "            json={\n",
    "                \"title\": f\"Load Test Incident {incident_num}\",\n",
    "                \"description\": \"Load testing\",\n",
    "                \"severity\": \"low\",\n",
    "                \"service_name\": \"load_test\",\n",
    "                \"auto_resolve\": False\n",
    "            }\n",
    "        ) as response:\n",
    "            return await response.json()\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [create_incident(session, i) for i in range(50)]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        successful = sum(1 for r in results if not isinstance(r, Exception))\n",
    "        print(f\"Successful requests: {successful}/50\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run manual tests\n",
    "    incident_id = manual_test_simple_incident()\n",
    "    if incident_id:\n",
    "        print(f\"Created incident: {incident_id}\")\n",
    "        \n",
    "    # Run load test\n",
    "    asyncio.run(load_test_api())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692939cd",
   "metadata": {},
   "source": [
    "## 11. Quick Reference {#quick-reference}\n",
    "\n",
    "### 11.1 Development Commands Cheatsheet\n",
    "\n",
    "```bash\n",
    "# Environment Setup\n",
    "python -m venv venv && source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "cp .env.example .env\n",
    "\n",
    "# Start Services\n",
    "docker-compose up -d postgres redis\n",
    "celery -A app.core.celery worker --loglevel=info\n",
    "uvicorn app.main:app --reload\n",
    "\n",
    "# Database\n",
    "alembic upgrade head\n",
    "alembic revision --autogenerate -m \"description\"\n",
    "\n",
    "# Testing\n",
    "pytest tests/ -v --cov=app\n",
    "pytest tests/api/ -k \"test_incident\"\n",
    "\n",
    "# Code Quality\n",
    "black app/ && isort app/ && flake8 app/\n",
    "```\n",
    "\n",
    "### 11.2 API Quick Reference\n",
    "\n",
    "| Endpoint | Method | Purpose |\n",
    "|----------|--------|---------|\n",
    "| `/api/v1/enhanced-incidents/` | POST | Create incident with auto-resolution |\n",
    "| `/api/v1/enhanced-incidents/{id}` | GET | Get incident details |\n",
    "| `/api/v1/ai/retrain` | POST | Trigger model retraining |\n",
    "| `/api/v1/ai/model-metrics` | GET | Get model performance |\n",
    "| `/api/v1/actions/execute` | POST | Execute manual action |\n",
    "| `/health` | GET | Health check |\n",
    "| `/docs` | GET | API documentation |\n",
    "\n",
    "### 11.3 Key Configuration Variables\n",
    "\n",
    "```bash\n",
    "# Core\n",
    "DATABASE_URL=postgresql://user:pass@host:5432/db\n",
    "REDIS_URL=redis://host:6379/0\n",
    "DEFAULT_DAG_ID=simple_test_dag\n",
    "\n",
    "# AI/ML\n",
    "OPENAI_API_KEY=your_key\n",
    "MIN_TRAINING_SAMPLES=100\n",
    "RETRAIN_THRESHOLD=0.85\n",
    "\n",
    "# External Services\n",
    "AIRFLOW_API_URL=http://airflow:8080/api/v1\n",
    "AIRFLOW_USERNAME=admin\n",
    "AIRFLOW_PASSWORD=password\n",
    "```\n",
    "\n",
    "### 11.4 Troubleshooting Guide\n",
    "\n",
    "#### Common Issues\n",
    "\n",
    "**Database Connection Errors**\n",
    "- Check `DATABASE_URL` format\n",
    "- Verify PostgreSQL is running\n",
    "- Check network connectivity\n",
    "\n",
    "**Celery Worker Issues**\n",
    "- Verify Redis is accessible\n",
    "- Check worker logs: `celery -A app.core.celery inspect active`\n",
    "- Restart workers if stuck\n",
    "\n",
    "**Model Training Failures**\n",
    "- Ensure minimum training samples exist\n",
    "- Check feature extraction pipeline\n",
    "- Verify model directory permissions\n",
    "\n",
    "**Airflow Integration Issues**\n",
    "- Test API connectivity: `curl $AIRFLOW_API_URL/health`\n",
    "- Verify credentials and permissions\n",
    "- Check DAG exists and is active\n",
    "\n",
    "### 11.5 Monitoring & Logs\n",
    "\n",
    "```bash\n",
    "# Application Logs\n",
    "tail -f logs/app.log\n",
    "\n",
    "# Docker Services\n",
    "docker-compose logs -f api worker beat\n",
    "\n",
    "# Database Queries\n",
    "psql $DATABASE_URL -c \"SELECT COUNT(*) FROM incidents;\"\n",
    "\n",
    "# Redis Queue Status\n",
    "redis-cli -u $REDIS_URL info\n",
    "\n",
    "# System Resources\n",
    "htop\n",
    "df -h\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This AI On-Call Agent system provides a comprehensive automation solution for ETL infrastructure monitoring and incident resolution. The system combines:\n",
    "\n",
    "- **Intelligent Analysis**: AI-powered incident classification and action recommendation\n",
    "- **Automated Resolution**: Seamless integration with external systems like Airflow\n",
    "- **Scalable Architecture**: Microservices design with async processing\n",
    "- **Production Ready**: Docker containerization, monitoring, and deployment guides\n",
    "\n",
    "The documentation covers all aspects from development setup to production deployment, ensuring teams can effectively implement, maintain, and extend the system.\n",
    "\n",
    "For additional support or questions, refer to the API documentation at `/docs` or check the project repository for the latest updates."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

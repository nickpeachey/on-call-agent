{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8793f46",
   "metadata": {},
   "source": [
    "# AI On-Call Agent - ML Training and Analysis (Fixed)\n",
    "\n",
    "This notebook demonstrates the machine learning capabilities of the AI On-Call Agent system.\n",
    "We'll train models, analyze performance, and test predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdced54",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook, ensure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "# Install required packages\n",
    "pip install matplotlib seaborn pandas numpy scikit-learn joblib\n",
    "```\n",
    "\n",
    "This notebook demonstrates the ML capabilities in a simplified, self-contained environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92627a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n",
      "üìÅ Working directory: /Users/nickpeachey/Developer/projects/on-call-agent/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from collections import Counter\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a021d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking required packages...\n",
      "  ‚úÖ matplotlib\n",
      "  ‚úÖ seaborn\n",
      "  ‚úÖ pandas\n",
      "  ‚úÖ numpy\n",
      "  ‚úÖ sklearn\n",
      "  ‚úÖ joblib\n",
      "\n",
      "‚úÖ All required packages are installed!\n",
      "üöÄ Ready to proceed with the demo!\n"
     ]
    }
   ],
   "source": [
    "# Verify required packages are installed\n",
    "required_packages = [\n",
    "    'matplotlib', 'seaborn', 'pandas', 'numpy', 'sklearn', 'joblib'\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "\n",
    "print(\"üîç Checking required packages...\")\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        if package == 'sklearn':\n",
    "            __import__('sklearn')\n",
    "        elif package == 'joblib':\n",
    "            __import__('joblib')\n",
    "        else:\n",
    "            __import__(package)\n",
    "        print(f\"  ‚úÖ {package}\")\n",
    "    except ImportError:\n",
    "        missing_packages.append(package)\n",
    "        print(f\"  ‚ùå {package}\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"Please install them with:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required packages are installed!\")\n",
    "    print(\"üöÄ Ready to proceed with the demo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b8dee",
   "metadata": {},
   "source": [
    "## 1. Initialize ML Service (Self-Contained)\n",
    "\n",
    "**Step 1: Create the ML service framework**\n",
    "\n",
    "This initializes the ML service class but doesn't train any models yet. The actual training happens in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee8f04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NotebookMLService initialized\n",
      "üîÑ ML Service created - checking for existing models...\n",
      "üìÅ Loaded incident classifier model\n",
      "üìÅ Loaded action recommender model\n",
      "üìÅ Loaded text vectorizer\n",
      "üìÅ Loaded model metadata\n",
      "‚úÖ Loaded existing trained models from disk!\n",
      "üöÄ ML Service ready with pre-trained models!\n"
     ]
    }
   ],
   "source": [
    "# Self-contained ML Service for notebook demo\n",
    "class NotebookMLService:\n",
    "    \"\"\"Simplified ML Service for notebook demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use models directory in current path\n",
    "        self.model_path = Path(\"../models\")\n",
    "        self.model_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.incident_classifier = None\n",
    "        self.action_recommender = None\n",
    "        self.text_vectorizer = None\n",
    "        \n",
    "        # Model metadata\n",
    "        self.model_metadata = {\n",
    "            \"incident_classifier\": {\"loaded\": False, \"accuracy\": 0.0, \"trained_at\": None},\n",
    "            \"action_recommender\": {\"loaded\": False, \"accuracy\": 0.0, \"trained_at\": None}\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ NotebookMLService initialized\")\n",
    "    \n",
    "    def load_models(self) -> bool:\n",
    "        \"\"\"Load trained models from disk.\"\"\"\n",
    "        try:\n",
    "            # Load incident classifier\n",
    "            incident_model_path = self.model_path / \"incident_classifier.joblib\"\n",
    "            if incident_model_path.exists():\n",
    "                self.incident_classifier = joblib.load(incident_model_path)\n",
    "                self.model_metadata[\"incident_classifier\"][\"loaded\"] = True\n",
    "                print(\"üìÅ Loaded incident classifier model\")\n",
    "            \n",
    "            # Load action recommender\n",
    "            action_model_path = self.model_path / \"action_recommender.joblib\"\n",
    "            if action_model_path.exists():\n",
    "                self.action_recommender = joblib.load(action_model_path)\n",
    "                self.model_metadata[\"action_recommender\"][\"loaded\"] = True\n",
    "                print(\"üìÅ Loaded action recommender model\")\n",
    "            \n",
    "            # Load vectorizer\n",
    "            text_vectorizer_path = self.model_path / \"text_vectorizer.joblib\"\n",
    "            if text_vectorizer_path.exists():\n",
    "                self.text_vectorizer = joblib.load(text_vectorizer_path)\n",
    "                print(\"üìÅ Loaded text vectorizer\")\n",
    "            \n",
    "            # Load metadata\n",
    "            metadata_path = self.model_path / \"model_metadata.json\"\n",
    "            if metadata_path.exists():\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    saved_metadata = json.load(f)\n",
    "                    self.model_metadata.update(saved_metadata)\n",
    "                print(\"üìÅ Loaded model metadata\")\n",
    "            \n",
    "            return self.incident_classifier is not None and self.action_recommender is not None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load models: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def save_models(self):\n",
    "        \"\"\"Save trained models to disk.\"\"\"\n",
    "        try:\n",
    "            # Save incident classifier\n",
    "            if self.incident_classifier is not None:\n",
    "                joblib.dump(self.incident_classifier, self.model_path / \"incident_classifier.joblib\")\n",
    "                print(\"üíæ Saved incident classifier\")\n",
    "            \n",
    "            # Save action recommender\n",
    "            if self.action_recommender is not None:\n",
    "                joblib.dump(self.action_recommender, self.model_path / \"action_recommender.joblib\")\n",
    "                print(\"üíæ Saved action recommender\")\n",
    "            \n",
    "            # Save vectorizer\n",
    "            if self.text_vectorizer is not None:\n",
    "                joblib.dump(self.text_vectorizer, self.model_path / \"text_vectorizer.joblib\")\n",
    "                print(\"üíæ Saved text vectorizer\")\n",
    "            \n",
    "            # Save metadata\n",
    "            with open(self.model_path / \"model_metadata.json\", 'w') as f:\n",
    "                json.dump(self.model_metadata, f, indent=2)\n",
    "                print(\"üíæ Saved model metadata\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save models: {e}\")\n",
    "    \n",
    "    def predict_incident_severity(self, incident_text: str):\n",
    "        \"\"\"Predict incident severity.\"\"\"\n",
    "        if not self.incident_classifier or not self.text_vectorizer:\n",
    "            return \"high\", 0.75  # Fallback\n",
    "        \n",
    "        try:\n",
    "            # Transform text using trained vectorizer\n",
    "            X = self.text_vectorizer.transform([incident_text])\n",
    "            \n",
    "            # Get prediction\n",
    "            severity = self.incident_classifier.predict(X)[0]\n",
    "            \n",
    "            # Get confidence from predict_proba\n",
    "            try:\n",
    "                proba = self.incident_classifier.predict_proba(X)[0]\n",
    "                confidence = max(proba)\n",
    "            except:\n",
    "                confidence = 0.8\n",
    "                \n",
    "            return severity, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Prediction error: {e}\")\n",
    "            return \"high\", 0.75\n",
    "    \n",
    "    def recommend_action(self, incident_text: str):\n",
    "        \"\"\"Recommend action for incident.\"\"\"\n",
    "        if not self.action_recommender or not self.text_vectorizer:\n",
    "            return \"restart_dag\", 0.8  # Fallback\n",
    "        \n",
    "        try:\n",
    "            # Transform text using trained vectorizer\n",
    "            X = self.text_vectorizer.transform([incident_text])\n",
    "            \n",
    "            # Get prediction\n",
    "            action = self.action_recommender.predict(X)[0]\n",
    "            \n",
    "            # Get confidence from predict_proba\n",
    "            try:\n",
    "                proba = self.action_recommender.predict_proba(X)[0]\n",
    "                confidence = max(proba)\n",
    "            except:\n",
    "                confidence = 0.8\n",
    "                \n",
    "            return action, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Action recommendation error: {e}\")\n",
    "            return \"restart_dag\", 0.8\n",
    "\n",
    "# Initialize the ML service and try to load existing models\n",
    "try:\n",
    "    ml_service = NotebookMLService()\n",
    "    print(\"üîÑ ML Service created - checking for existing models...\")\n",
    "    \n",
    "    # Try to load existing models first\n",
    "    if ml_service.load_models():\n",
    "        print(\"‚úÖ Loaded existing trained models from disk!\")\n",
    "        print(\"üöÄ ML Service ready with pre-trained models!\")\n",
    "    else:\n",
    "        print(\"üìù No existing models found - will train new models in the next section\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing ML service: {e}\")\n",
    "    print(\"Creating minimal fallback service...\")\n",
    "    \n",
    "    class FallbackMLService:\n",
    "        def predict_incident_severity(self, text):\n",
    "            return \"high\", 0.85\n",
    "        \n",
    "        def recommend_action(self, text):\n",
    "            return \"restart_dag\", 0.88\n",
    "    \n",
    "    ml_service = FallbackMLService()\n",
    "    print(\"‚úÖ Fallback service ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32404ce",
   "metadata": {},
   "source": [
    "## 2. Train Models with Real Data\n",
    "\n",
    "**This is where we actually create and train the ML models.**\n",
    "\n",
    "The ML service was initialized above but no models exist yet. Here we'll:\n",
    "1. Load training data\n",
    "2. Process features \n",
    "3. Train the incident classifier and action recommender\n",
    "4. Save the trained models for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28861ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING ML TRAINING WITH PROGRESS TRACKING\n",
      "==================================================\n",
      "‚è∞ Started at: 11:18:49\n",
      "‚úÖ Models already trained and loaded!\n",
      "üìä Model performance:\n",
      "  incident_classifier: 0.700 accuracy (2000 samples)\n",
      "  action_recommender: 0.797 accuracy (2000 samples)\n",
      "üéØ Skipping training - models are ready for use!\n",
      "üí° To retrain models, restart the kernel and run without loading existing models\n",
      "\n",
      "üß™ Quick test with loaded models:\n",
      "  'DAG payment_processing failed with timeo...' ‚Üí high (0.68) | restart_service (0.53)\n",
      "  'Spark job out of memory error...' ‚Üí high (0.59) | scale_up (0.65)\n",
      "\n",
      "‚ú® LOADED MODELS READY FOR USE!\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ML Training with Progress Indicators\n",
    "print(\"üöÄ STARTING ML TRAINING WITH PROGRESS TRACKING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚è∞ Started at: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Check if models are already trained and loaded\n",
    "if (ml_service.incident_classifier is not None and \n",
    "    ml_service.action_recommender is not None and \n",
    "    ml_service.text_vectorizer is not None):\n",
    "    print(\"‚úÖ Models already trained and loaded!\")\n",
    "    print(\"üìä Model performance:\")\n",
    "    if hasattr(ml_service, 'model_metadata'):\n",
    "        for model_name, metadata in ml_service.model_metadata.items():\n",
    "            if metadata.get('loaded', False):\n",
    "                acc = metadata.get('accuracy', 0)\n",
    "                samples = metadata.get('training_samples', 0)\n",
    "                print(f\"  {model_name}: {acc:.3f} accuracy ({samples} samples)\")\n",
    "    print(\"üéØ Skipping training - models are ready for use!\")\n",
    "    print(\"üí° To retrain models, restart the kernel and run without loading existing models\")\n",
    "    \n",
    "    # Quick test of loaded models\n",
    "    print(f\"\\nüß™ Quick test with loaded models:\")\n",
    "    test_scenarios = [\n",
    "        \"DAG payment_processing failed with timeout\",\n",
    "        \"Spark job out of memory error\"\n",
    "    ]\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        severity, sev_conf = ml_service.predict_incident_severity(scenario)\n",
    "        action, act_conf = ml_service.recommend_action(scenario)\n",
    "        print(f\"  '{scenario[:40]}...' ‚Üí {severity} ({sev_conf:.2f}) | {action} ({act_conf:.2f})\")\n",
    "    \n",
    "    print(f\"\\n‚ú® LOADED MODELS READY FOR USE!\")\n",
    "    \n",
    "    # Set skip_training flag\n",
    "    skip_training = True\n",
    "else:\n",
    "    print(\"üîÑ No trained models found - proceeding with training...\")\n",
    "    skip_training = False\n",
    "\n",
    "# Only run training if models are not already loaded\n",
    "if not skip_training:\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # STEP 1: Load data\n",
    "        print(\"\\nüìÅ STEP 1: Loading training data...\")\n",
    "        data_file = Path(\"../data/comprehensive_training.json\")\n",
    "        \n",
    "        if not data_file.exists():\n",
    "            print(\"‚ùå Training data not found!\")\n",
    "            print(\"   Creating sample data for demo...\")\n",
    "            \n",
    "            # Create sample training data if file doesn't exist\n",
    "            sample_data = [\n",
    "                {\"incident\": \"DAG payment_processing failed with timeout\", \"severity\": \"high\", \"action\": \"restart_dag\"},\n",
    "                {\"incident\": \"Spark job out of memory error\", \"severity\": \"critical\", \"action\": \"scale_up\"},\n",
    "                {\"incident\": \"Database connection timeout\", \"severity\": \"medium\", \"action\": \"restart_service\"},\n",
    "                {\"incident\": \"ETL pipeline stuck for 2 hours\", \"severity\": \"high\", \"action\": \"restart_dag\"},\n",
    "                {\"incident\": \"Airflow worker node high CPU usage\", \"severity\": \"medium\", \"action\": \"scale_up\"},\n",
    "                {\"incident\": \"Data warehouse sync failed\", \"severity\": \"high\", \"action\": \"check_logs\"},\n",
    "                {\"incident\": \"API rate limit exceeded\", \"severity\": \"low\", \"action\": \"wait_and_retry\"},\n",
    "                {\"incident\": \"Disk space full on server\", \"severity\": \"critical\", \"action\": \"cleanup_disk\"},\n",
    "                {\"incident\": \"Network connectivity issues\", \"severity\": \"high\", \"action\": \"check_network\"},\n",
    "                {\"incident\": \"Service health check failed\", \"severity\": \"medium\", \"action\": \"restart_service\"}\n",
    "            ] * 100  # Repeat for more training data\n",
    "            \n",
    "            training_data = sample_data\n",
    "            print(f\"üìä Created {len(training_data)} sample training examples\")\n",
    "        else:\n",
    "            load_start = time.time()\n",
    "            with open(data_file, 'r') as f:\n",
    "                training_data = json.load(f)\n",
    "            load_time = time.time() - load_start\n",
    "            \n",
    "            print(f\"üìä Loaded {len(training_data)} real training examples in {load_time:.2f}s\")\n",
    "        \n",
    "        # Use smaller subset for faster demo\n",
    "        if len(training_data) > 2000:\n",
    "            print(f\"üéØ Using 2000 examples for faster notebook demo\")\n",
    "            import random\n",
    "            random.seed(42)\n",
    "            training_data = random.sample(training_data, 2000)\n",
    "        \n",
    "        # STEP 2: Process features\n",
    "        print(f\"\\nüîß STEP 2: Processing {len(training_data)} examples...\")\n",
    "        feature_start = time.time()\n",
    "        \n",
    "        incidents = []\n",
    "        severities = []\n",
    "        actions = []\n",
    "        \n",
    "        for i, item in enumerate(training_data):\n",
    "            if i % 500 == 0 and i > 0:\n",
    "                print(f\"   ‚ö° Processed {i}/{len(training_data)} examples ({i/len(training_data)*100:.0f}%)\")\n",
    "            \n",
    "            incident_text = item['incident']\n",
    "            \n",
    "            # Add infrastructure context if available\n",
    "            if 'infrastructure' in item:\n",
    "                infra = item['infrastructure']\n",
    "                if 'dag_id' in infra:\n",
    "                    incident_text += f\" dag_id:{infra['dag_id']}\"\n",
    "                if 'server_name' in infra:\n",
    "                    incident_text += f\" server:{infra['server_name']}\"\n",
    "            \n",
    "            incidents.append(incident_text)\n",
    "            severities.append(item['severity'])\n",
    "            actions.append(item['action'])\n",
    "        \n",
    "        feature_time = time.time() - feature_start\n",
    "        print(f\"‚úÖ Feature processing completed in {feature_time:.2f}s\")\n",
    "        \n",
    "        # STEP 3: Vectorize\n",
    "        print(f\"\\nüìä STEP 3: Creating text features...\")\n",
    "        vectorizer_start = time.time()\n",
    "        \n",
    "        ml_service.text_vectorizer = TfidfVectorizer(\n",
    "            max_features=500, \n",
    "            stop_words='english', \n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2\n",
    "        )\n",
    "        X = ml_service.text_vectorizer.fit_transform(incidents)\n",
    "        \n",
    "        vectorizer_time = time.time() - vectorizer_start\n",
    "        print(f\"‚úÖ Vectorized to {X.shape[0]} samples x {X.shape[1]} features in {vectorizer_time:.2f}s\")\n",
    "        \n",
    "        # STEP 4: Split data\n",
    "        print(f\"\\nüîÄ STEP 4: Splitting data...\")\n",
    "        split_start = time.time()\n",
    "        \n",
    "        X_train, X_test, y_sev_train, y_sev_test, y_act_train, y_act_test = train_test_split(\n",
    "            X, severities, actions, test_size=0.2, random_state=42, stratify=severities\n",
    "        )\n",
    "        \n",
    "        split_time = time.time() - split_start\n",
    "        print(f\"‚úÖ Split completed in {split_time:.2f}s\")\n",
    "        print(f\"   Training: {X_train.shape[0]} samples | Testing: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # STEP 5: Train models\n",
    "        print(f\"\\nü§ñ STEP 5: Training models...\")\n",
    "        print(\"   üéØ Training incident classifier (RandomForest with 50 trees)...\")\n",
    "        \n",
    "        classifier_start = time.time()\n",
    "        ml_service.incident_classifier = RandomForestClassifier(\n",
    "            n_estimators=50, \n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        ml_service.incident_classifier.fit(X_train, y_sev_train)\n",
    "        classifier_time = time.time() - classifier_start\n",
    "        \n",
    "        print(f\"‚úÖ Incident classifier trained in {classifier_time:.2f}s\")\n",
    "        \n",
    "        print(\"   üîß Training action recommender (RandomForest with 50 trees)...\")\n",
    "        recommender_start = time.time()\n",
    "        ml_service.action_recommender = RandomForestClassifier(\n",
    "            n_estimators=50, \n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        ml_service.action_recommender.fit(X_train, y_act_train)\n",
    "        recommender_time = time.time() - recommender_start\n",
    "        \n",
    "        print(f\"‚úÖ Action recommender trained in {recommender_time:.2f}s\")\n",
    "        \n",
    "        # STEP 6: Evaluate\n",
    "        print(f\"\\nüìà STEP 6: Evaluating models...\")\n",
    "        eval_start = time.time()\n",
    "        \n",
    "        severity_predictions = ml_service.incident_classifier.predict(X_test)\n",
    "        severity_accuracy = accuracy_score(y_sev_test, severity_predictions)\n",
    "        \n",
    "        action_predictions = ml_service.action_recommender.predict(X_test)\n",
    "        action_accuracy = accuracy_score(y_act_test, action_predictions)\n",
    "        \n",
    "        eval_time = time.time() - eval_start\n",
    "        print(f\"‚úÖ Evaluation completed in {eval_time:.2f}s\")\n",
    "        \n",
    "        # Update metadata\n",
    "        ml_service.model_metadata['incident_classifier'] = {\n",
    "            'loaded': True,\n",
    "            'accuracy': severity_accuracy,\n",
    "            'trained_at': datetime.now(timezone.utc).isoformat(),\n",
    "            'training_samples': len(incidents),\n",
    "            'features': X.shape[1]\n",
    "        }\n",
    "        \n",
    "        ml_service.model_metadata['action_recommender'] = {\n",
    "            'loaded': True,\n",
    "            'accuracy': action_accuracy,\n",
    "            'trained_at': datetime.now(timezone.utc).isoformat(),\n",
    "            'training_samples': len(incidents),\n",
    "            'features': X.shape[1]\n",
    "        }\n",
    "        \n",
    "        # STEP 7: Save models\n",
    "        print(f\"\\nüíæ STEP 7: Saving models...\")\n",
    "        save_start = time.time()\n",
    "        ml_service.save_models()\n",
    "        save_time = time.time() - save_start\n",
    "        print(f\"‚úÖ Models saved in {save_time:.2f}s\")\n",
    "        \n",
    "        # STEP 8: Results\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nüéâ TRAINING COMPLETED!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"‚è∞ Total time: {total_time:.2f}s ({total_time/60:.1f} minutes)\")\n",
    "        print(f\"üìä Dataset: {len(training_data)} examples\")\n",
    "        print(f\"üéØ Severity accuracy: {severity_accuracy:.3f}\")\n",
    "        print(f\"üîß Action accuracy: {action_accuracy:.3f}\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Performance breakdown:\")\n",
    "        print(f\"  üìÅ Data loading: {feature_time:.2f}s\")\n",
    "        print(f\"  üîß Feature processing: {feature_time:.2f}s\")\n",
    "        print(f\"  üìä Vectorization: {vectorizer_time:.2f}s\")\n",
    "        print(f\"  üîÄ Data splitting: {split_time:.2f}s\")\n",
    "        print(f\"  üéØ Classifier training: {classifier_time:.2f}s\")\n",
    "        print(f\"  üîß Recommender training: {recommender_time:.2f}s\")\n",
    "        print(f\"  üìà Evaluation: {eval_time:.2f}s\")\n",
    "        print(f\"  üíæ Model saving: {save_time:.2f}s\")\n",
    "        \n",
    "        # Show data analysis\n",
    "        severity_counts = Counter(severities)\n",
    "        action_counts = Counter(actions)\n",
    "        \n",
    "        print(f\"\\nüìä Data Analysis:\")\n",
    "        print(f\"  Severities: {dict(severity_counts)}\")\n",
    "        print(f\"  Top actions: {dict(action_counts.most_common(5))}\")\n",
    "        \n",
    "        # Test predictions\n",
    "        print(f\"\\nüß™ Quick test predictions:\")\n",
    "        test_scenarios = [\n",
    "            \"DAG payment_processing failed with timeout\",\n",
    "            \"Spark job out of memory error\",\n",
    "            \"Database connection timeout\"\n",
    "        ]\n",
    "        \n",
    "        for scenario in test_scenarios:\n",
    "            severity, sev_conf = ml_service.predict_incident_severity(scenario)\n",
    "            action, act_conf = ml_service.recommend_action(scenario)\n",
    "            print(f\"  '{scenario[:40]}...' ‚Üí {severity} ({sev_conf:.2f}) | {action} ({act_conf:.2f})\")\n",
    "        \n",
    "        print(f\"\\n‚ú® TRAINING SUCCESS! Models are ready!\")\n",
    "        print(f\"üïê Finished at: {time.strftime('%H:%M:%S')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# End of training\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ac6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying trained models...\n",
      "‚úÖ Incident classifier is trained and ready\n",
      "‚úÖ Action recommender is trained and ready\n",
      "‚úÖ Text vectorizer is trained and ready\n",
      "\n",
      "üìä Model metadata:\n",
      "  incident_classifier: 0.700 accuracy (2000 samples)\n",
      "  action_recommender: 0.797 accuracy (2000 samples)\n",
      "\n",
      "üöÄ Models are ready for predictions!\n"
     ]
    }
   ],
   "source": [
    "# Verify models are now trained and loaded\n",
    "print(\"üîç Verifying trained models...\")\n",
    "\n",
    "if ml_service.incident_classifier is not None:\n",
    "    print(\"‚úÖ Incident classifier is trained and ready\")\n",
    "else:\n",
    "    print(\"‚ùå Incident classifier is missing\")\n",
    "\n",
    "if ml_service.action_recommender is not None:\n",
    "    print(\"‚úÖ Action recommender is trained and ready\")\n",
    "else:\n",
    "    print(\"‚ùå Action recommender is missing\")\n",
    "\n",
    "if ml_service.text_vectorizer is not None:\n",
    "    print(\"‚úÖ Text vectorizer is trained and ready\")\n",
    "else:\n",
    "    print(\"‚ùå Text vectorizer is missing\")\n",
    "\n",
    "print(f\"\\nüìä Model metadata:\")\n",
    "for model_name, metadata in ml_service.model_metadata.items():\n",
    "    if metadata.get('loaded', False):\n",
    "        acc = metadata.get('accuracy', 0)\n",
    "        samples = metadata.get('training_samples', 0)\n",
    "        print(f\"  {model_name}: {acc:.3f} accuracy ({samples} samples)\")\n",
    "\n",
    "print(\"\\nüöÄ Models are ready for predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09c5c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing model loading from disk...\n",
      "========================================\n",
      "‚úÖ NotebookMLService initialized\n",
      "üìù New ML service created (models not loaded yet)\n",
      "üìÅ Loaded incident classifier model\n",
      "üìÅ Loaded action recommender model\n",
      "üìÅ Loaded text vectorizer\n",
      "üìÅ Loaded model metadata\n",
      "‚úÖ Successfully loaded models from disk!\n",
      "üìä Loaded models can make predictions:\n",
      "  Test prediction: 'DAG payment_processing failed with timeout'\n",
      "  ‚Üí Severity: high (confidence: 0.679)\n",
      "  ‚Üí Action: restart_service (confidence: 0.530)\n",
      "\n",
      "üí° This demonstrates that models persist correctly to disk!\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Test loading models from disk (simulating a fresh start)\n",
    "print(\"üîÑ Testing model loading from disk...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a new ML service instance to test loading\n",
    "test_service = NotebookMLService()\n",
    "print(\"üìù New ML service created (models not loaded yet)\")\n",
    "\n",
    "# Try to load the saved models\n",
    "loaded_successfully = test_service.load_models()\n",
    "\n",
    "if loaded_successfully:\n",
    "    print(\"‚úÖ Successfully loaded models from disk!\")\n",
    "    print(f\"üìä Loaded models can make predictions:\")\n",
    "    \n",
    "    # Test a prediction with the loaded models\n",
    "    test_incident = \"DAG payment_processing failed with timeout\"\n",
    "    severity, sev_conf = test_service.predict_incident_severity(test_incident)\n",
    "    action, act_conf = test_service.recommend_action(test_incident)\n",
    "    \n",
    "    print(f\"  Test prediction: '{test_incident}'\")\n",
    "    print(f\"  ‚Üí Severity: {severity} (confidence: {sev_conf:.3f})\")\n",
    "    print(f\"  ‚Üí Action: {action} (confidence: {act_conf:.3f})\")\n",
    "    \n",
    "    print(\"\\nüí° This demonstrates that models persist correctly to disk!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load models from disk\")\n",
    "    print(\"   Models may not have been saved yet - run the training cell first\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c854146",
   "metadata": {},
   "source": [
    "## 3. Test Predictions\n",
    "\n",
    "Test the trained models with various incident scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bf387e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Airflow/ETL Incident Predictions\n",
      "Incident Description                                    Severity   Confidence\n",
      "---------------------------------------------------------------------------\n",
      "Airflow DAG customer_analytics failed with timeout      medium     0.503\n",
      "ETL pipeline daily_sales_report has been stuck for      high       0.771\n",
      "Data warehouse refresh DAG failed with memory exce      high       0.604\n",
      "Critical reporting DAG payment_reconciliation fail      high       0.691\n",
      "DAG user_segmentation failed but restart worked ye      high       0.695\n",
      "ETL job inventory_sync intermittent failure - rest      high       0.737\n",
      "Spark job real_time_processing out of memory            high       0.711\n",
      "Scala ETL application data_transformer crashed          high       0.820\n",
      "High CPU usage on Airflow worker node                   high       0.859\n",
      "Database connection timeout in analytics pipeline       high       0.704\n",
      "\n",
      "üìà Airflow/ETL predictions collected: 10\n",
      "\n",
      "üìä Severity Distribution:\n",
      "  high: 9 incidents (90%)\n",
      "  medium: 1 incidents (10%)\n"
     ]
    }
   ],
   "source": [
    "# Test various Airflow and ETL incident scenarios\n",
    "test_incidents = [\n",
    "    # Airflow DAG failures\n",
    "    \"Airflow DAG customer_analytics failed with timeout error\",\n",
    "    \"ETL pipeline daily_sales_report has been stuck for 2 hours\", \n",
    "    \"Data warehouse refresh DAG failed with memory exception\",\n",
    "    \"Critical reporting DAG payment_reconciliation failed\",\n",
    "    \n",
    "    # Restart scenarios\n",
    "    \"DAG user_segmentation failed but restart worked yesterday\",\n",
    "    \"ETL job inventory_sync intermittent failure - restart usually fixes\",\n",
    "    \n",
    "    # Spark/Scala monitoring\n",
    "    \"Spark job real_time_processing out of memory\",\n",
    "    \"Scala ETL application data_transformer crashed\",\n",
    "    \n",
    "    # Infrastructure supporting ETL\n",
    "    \"High CPU usage on Airflow worker node\",\n",
    "    \"Database connection timeout in analytics pipeline\"\n",
    "]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "print(\"üîç Testing Airflow/ETL Incident Predictions\")\n",
    "print(f\"{'Incident Description':<55} {'Severity':<10} {'Confidence':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for incident in test_incidents:\n",
    "    severity, confidence = ml_service.predict_incident_severity(incident)\n",
    "    predictions.append({\n",
    "        'incident': incident,\n",
    "        'severity': severity,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "    \n",
    "    print(f\"{incident[:50]:<55} {severity:<10} {confidence:.3f}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "print(f\"\\nüìà Airflow/ETL predictions collected: {len(df_predictions)}\")\n",
    "\n",
    "# Show severity distribution\n",
    "severity_counts = df_predictions['severity'].value_counts()\n",
    "print(f\"\\nüìä Severity Distribution:\")\n",
    "for severity, count in severity_counts.items():\n",
    "    print(f\"  {severity}: {count} incidents ({count/len(df_predictions)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2280314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Testing Airflow/ETL Action Recommendations\n",
      "Incident Description                                    Action          Confidence\n",
      "--------------------------------------------------------------------------------\n",
      "Airflow DAG customer_analytics failed with timeout      restart_dag     0.340\n",
      "ETL pipeline daily_sales_report has been stuck for      restart_service 0.684\n",
      "Data warehouse refresh DAG failed with memory exce      scale_up        0.440\n",
      "Critical reporting DAG payment_reconciliation fail      restart_service 0.568\n",
      "DAG user_segmentation failed but restart worked ye      restart_service 0.588\n",
      "ETL job inventory_sync intermittent failure - rest      restart_service 0.606\n",
      "Spark job real_time_processing out of memory            scale_up        0.553\n",
      "Scala ETL application data_transformer crashed          restart_service 0.754\n",
      "High CPU usage on Airflow worker node                   scale_up        0.660\n",
      "Database connection timeout in analytics pipeline       restart_service 0.658\n",
      "\n",
      "üéØ Action recommendations collected: 10\n",
      "\n",
      "üîß Action Distribution:\n",
      "  restart_service: 6 incidents (60%)\n",
      "  scale_up: 3 incidents (30%)\n",
      "  restart_dag: 1 incidents (10%)\n",
      "\n",
      "üîÑ Airflow Insights:\n",
      "  ‚Ä¢ 1 incidents recommend DAG restart\n",
      "  ‚Ä¢ Model learned DAG restart patterns from training data\n",
      "  ‚Ä¢ Appropriate for transient DAG failures\n"
     ]
    }
   ],
   "source": [
    "# Test action recommendations for Airflow/ETL incidents\n",
    "action_recommendations = []\n",
    "\n",
    "print(\"üéØ Testing Airflow/ETL Action Recommendations\")\n",
    "print(f\"{'Incident Description':<55} {'Action':<15} {'Confidence':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for incident in test_incidents:\n",
    "    action, confidence = ml_service.recommend_action(incident)\n",
    "    action_recommendations.append({\n",
    "        'incident': incident,\n",
    "        'action': action,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "    \n",
    "    print(f\"{incident[:50]:<55} {action:<15} {confidence:.3f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_actions = pd.DataFrame(action_recommendations)\n",
    "print(f\"\\nüéØ Action recommendations collected: {len(df_actions)}\")\n",
    "\n",
    "# Show action distribution\n",
    "action_counts = df_actions['action'].value_counts()\n",
    "print(f\"\\nüîß Action Distribution:\")\n",
    "for action, count in action_counts.items():\n",
    "    print(f\"  {action}: {count} incidents ({count/len(df_actions)*100:.0f}%)\")\n",
    "\n",
    "# Highlight Airflow-specific insights\n",
    "restart_dag_count = len(df_actions[df_actions['action'] == 'restart_dag'])\n",
    "if restart_dag_count > 0:\n",
    "    print(f\"\\nüîÑ Airflow Insights:\")\n",
    "    print(f\"  ‚Ä¢ {restart_dag_count} incidents recommend DAG restart\")\n",
    "    print(f\"  ‚Ä¢ Model learned DAG restart patterns from training data\")\n",
    "    print(f\"  ‚Ä¢ Appropriate for transient DAG failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a1888",
   "metadata": {},
   "source": [
    "## 4. Visualize Results\n",
    "\n",
    "Create visualizations of the prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ef968",
   "metadata": {},
   "source": [
    "## 5. Model Performance Summary\n",
    "\n",
    "Summary of the ML training and performance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6243357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã ML Training Demo Summary\n",
      "==================================================\n",
      "\n",
      "üéØ Successfully Demonstrated:\n",
      "  ‚úÖ ML Service initialization and setup\n",
      "  ‚úÖ Model training with real/sample data\n",
      "  ‚úÖ Incident severity predictions (10 test cases)\n",
      "  ‚úÖ Action recommendations (10 test cases)\n",
      "  ‚úÖ Data visualization and analysis\n",
      "  ‚úÖ Model persistence (save/load)\n",
      "\n",
      "üìä Model Performance:\n",
      "  üéØ Incident Classifier Accuracy: 0.700\n",
      "  üîß Action Recommender Accuracy: 0.797\n",
      "\n",
      "üìä Prediction Analysis:\n",
      "  ‚Ä¢ Most common severity: high (90.0%)\n",
      "  ‚Ä¢ Most recommended action: restart_service (60.0%)\n",
      "  ‚Ä¢ Average severity confidence: 0.709\n",
      "  ‚Ä¢ Average action confidence: 0.585\n",
      "\n",
      "üí° Key Features:\n",
      "  ‚úÖ RandomForest models for robust predictions\n",
      "  ‚úÖ TF-IDF vectorization with bigrams\n",
      "  ‚úÖ Infrastructure context integration\n",
      "  ‚úÖ Joblib model persistence\n",
      "  ‚úÖ Comprehensive error handling\n",
      "\n",
      "üéâ Demo completed successfully!\n",
      "üìö The AI On-Call Agent ML system demonstrates core functionality.\n",
      "üöÄ Ready for integration with live incident data!\n"
     ]
    }
   ],
   "source": [
    "# Generate summary insights\n",
    "print(\"üìã ML Training Demo Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüéØ Successfully Demonstrated:\")\n",
    "print(f\"  ‚úÖ ML Service initialization and setup\")\n",
    "print(f\"  ‚úÖ Model training with real/sample data\")\n",
    "print(f\"  ‚úÖ Incident severity predictions ({len(df_predictions)} test cases)\")\n",
    "print(f\"  ‚úÖ Action recommendations ({len(df_actions)} test cases)\")\n",
    "print(f\"  ‚úÖ Data visualization and analysis\")\n",
    "print(f\"  ‚úÖ Model persistence (save/load)\")\n",
    "\n",
    "print(f\"\\nüìä Model Performance:\")\n",
    "if hasattr(ml_service, 'model_metadata'):\n",
    "    incident_acc = ml_service.model_metadata.get('incident_classifier', {}).get('accuracy', 0)\n",
    "    action_acc = ml_service.model_metadata.get('action_recommender', {}).get('accuracy', 0)\n",
    "    print(f\"  üéØ Incident Classifier Accuracy: {incident_acc:.3f}\")\n",
    "    print(f\"  üîß Action Recommender Accuracy: {action_acc:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Prediction Analysis:\")\n",
    "severity_distribution = df_predictions['severity'].value_counts(normalize=True)\n",
    "action_distribution = df_actions['action'].value_counts(normalize=True)\n",
    "\n",
    "print(f\"  ‚Ä¢ Most common severity: {severity_distribution.index[0]} ({severity_distribution.iloc[0]:.1%})\")\n",
    "print(f\"  ‚Ä¢ Most recommended action: {action_distribution.index[0]} ({action_distribution.iloc[0]:.1%})\")\n",
    "print(f\"  ‚Ä¢ Average severity confidence: {df_predictions['confidence'].mean():.3f}\")\n",
    "print(f\"  ‚Ä¢ Average action confidence: {df_actions['confidence'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Key Features:\")\n",
    "print(f\"  ‚úÖ RandomForest models for robust predictions\")\n",
    "print(f\"  ‚úÖ TF-IDF vectorization with bigrams\")\n",
    "print(f\"  ‚úÖ Infrastructure context integration\")\n",
    "print(f\"  ‚úÖ Joblib model persistence\")\n",
    "print(f\"  ‚úÖ Comprehensive error handling\")\n",
    "\n",
    "print(f\"\\nüéâ Demo completed successfully!\")\n",
    "print(f\"üìö The AI On-Call Agent ML system demonstrates core functionality.\")\n",
    "print(f\"üöÄ Ready for integration with live incident data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
